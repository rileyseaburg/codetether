{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c739339d",
   "metadata": {},
   "source": [
    "# ðŸ§  QuantumHead â€” 3DGS Talking Head Avatar Pipeline\n",
    "\n",
    "**Train on Colab (A100) â†’ Push weights to spike2 â†’ Serve inference from cloud**\n",
    "\n",
    "Architecture based on SOTA research:\n",
    "- **GAGAvatar** (246 FPS one-shot) â€” UV-space Gaussians + PanoHead GAN\n",
    "- **GaussianHeadTalk** (wobble-free) â€” Audio â†’ FLAME params via transformer\n",
    "- **UHAP** (universal prior) â€” Expression latent encodes geometry + appearance\n",
    "\n",
    "Pipeline:\n",
    "```\n",
    "Image â†’ DECA (FLAME fit) â†’ PanoHead (full-head tri-plane) â†’ UV Gaussians\n",
    "Audio â†’ Wav2Vec2 â†’ Transformer â†’ FLAME params â†’ Animate â†’ 3DGS Render\n",
    "                                                              â†“\n",
    "                                              Push weights to spike2\n",
    "                                              spike2 serves inference\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277b0b4a",
   "metadata": {},
   "source": [
    "## 0. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c8faf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION â€” Set these before running\n",
    "# ============================================================\n",
    "\n",
    "# spike2 server for inference serving\n",
    "SPIKE2_HOST = \"voice.quantum-forge.io\"  # Public URL\n",
    "SPIKE2_SSH = \"spike2\"                   # SSH alias (must be in ~/.ssh/config)\n",
    "SPIKE2_WEIGHTS_DIR = \"/root/quantumhead/weights\"\n",
    "SPIKE2_API_PORT = 8000\n",
    "\n",
    "# Training config\n",
    "BATCH_SIZE = 4\n",
    "NUM_GAUSSIANS = 256_000   # No VRAM myth â€” full 256K\n",
    "UV_MAP_SIZE = 256         # KÃ—K UV attribute maps\n",
    "EXPRESSION_DIM = 256      # Z_exp latent dimension\n",
    "IDENTITY_DIM = 512        # Z_id latent dimension\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_ITERATIONS = 50_000   # Start with 50K, scale to 300K\n",
    "GUIDE_MESH_VERTICES = 7306\n",
    "\n",
    "# Diffusion config (audio â†’ expression)\n",
    "DIFFUSION_STEPS = 500\n",
    "AUDIO_CONTEXT_WINDOW = 120  # frames\n",
    "AUDIO_OVERLAP = 30          # frames\n",
    "\n",
    "# Model output\n",
    "OUTPUT_DIR = \"/content/quantumhead_output\"\n",
    "CHECKPOINT_DIR = f\"{OUTPUT_DIR}/checkpoints\"\n",
    "\n",
    "print(\"âœ“ Configuration set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1e700f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GPU CHECK\n",
    "# ============================================================\n",
    "import subprocess\n",
    "result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")\n",
    "    assert torch.cuda.get_device_properties(0).total_mem > 30e9, \"Need A100 (40/80GB). Change runtime!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3632b4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INSTALL DEPENDENCIES\n",
    "# ============================================================\n",
    "%%bash\n",
    "set -e\n",
    "\n",
    "# Core ML\n",
    "pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# 3D / Rendering\n",
    "pip install -q \\\n",
    "  pytorch3d \\\n",
    "  trimesh \\\n",
    "  pyrender \\\n",
    "  open3d \\\n",
    "  plyfile\n",
    "\n",
    "# FLAME / Face\n",
    "pip install -q \\\n",
    "  chumpy \\\n",
    "  face-alignment \\\n",
    "  mediapipe \\\n",
    "  insightface \\\n",
    "  onnxruntime-gpu\n",
    "\n",
    "# Audio\n",
    "pip install -q \\\n",
    "  transformers \\\n",
    "  librosa \\\n",
    "  soundfile\n",
    "\n",
    "# Diffusion\n",
    "pip install -q \\\n",
    "  diffusers \\\n",
    "  accelerate\n",
    "\n",
    "# Gaussian Splatting (build from source)\n",
    "pip install -q \\\n",
    "  gsplat\n",
    "\n",
    "# Utils\n",
    "pip install -q \\\n",
    "  opencv-python-headless \\\n",
    "  scikit-image \\\n",
    "  einops \\\n",
    "  lpips \\\n",
    "  paramiko \\\n",
    "  scp \\\n",
    "  tqdm \\\n",
    "  wandb\n",
    "\n",
    "echo \"âœ“ All dependencies installed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d08a179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CLONE COMPONENT REPOS\n",
    "# ============================================================\n",
    "import os\n",
    "os.makedirs('/content/repos', exist_ok=True)\n",
    "os.chdir('/content/repos')\n",
    "\n",
    "repos = {\n",
    "    'gaussian-avatars': 'https://github.com/ShenhanQian/GaussianAvatars.git',\n",
    "    'DECA': 'https://github.com/yfeng95/DECA.git',\n",
    "    'FaceFormer': 'https://github.com/EvelynFan/FaceFormer.git',\n",
    "}\n",
    "\n",
    "for name, url in repos.items():\n",
    "    if not os.path.exists(name):\n",
    "        os.system(f'git clone --depth 1 {url} {name}')\n",
    "        print(f'âœ“ Cloned {name}')\n",
    "    else:\n",
    "        print(f'âœ“ {name} already exists')\n",
    "\n",
    "os.chdir('/content')\n",
    "print('\\nâœ“ All repos ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cef8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DOWNLOAD PRETRAINED WEIGHTS\n",
    "# ============================================================\n",
    "import os\n",
    "os.makedirs('/content/weights', exist_ok=True)\n",
    "\n",
    "# FLAME model (requires registration at https://flame.is.tue.mpg.de/)\n",
    "# Upload generic_model.pkl manually or mount from Drive\n",
    "FLAME_MODEL_PATH = '/content/weights/generic_model.pkl'\n",
    "\n",
    "# DECA pretrained\n",
    "DECA_CKPT = '/content/weights/deca_model.tar'\n",
    "\n",
    "# Wav2Vec2 (auto-downloaded by transformers)\n",
    "WAV2VEC_MODEL = 'facebook/wav2vec2-large-960h'\n",
    "\n",
    "# Check what we have\n",
    "print('Weight files:')\n",
    "for f in os.listdir('/content/weights'):\n",
    "    size = os.path.getsize(f'/content/weights/{f}') / 1e6\n",
    "    print(f'  {f}: {size:.1f} MB')\n",
    "\n",
    "if not os.path.exists(FLAME_MODEL_PATH):\n",
    "    print('\\nâš ï¸  FLAME model not found. Upload generic_model.pkl to /content/weights/')\n",
    "    print('   Get it from: https://flame.is.tue.mpg.de/')\n",
    "    print('   Or mount Google Drive with: drive.mount(\"/content/drive\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dd62e0",
   "metadata": {},
   "source": [
    "## 1. FLAME Parametric Head Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b62fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FLAME MODEL WRAPPER\n",
    "# ============================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "class FLAMEModel(nn.Module):\n",
    "    \"\"\"FLAME parametric head model.\n",
    "\n",
    "    shape(Î²): 300-dim identity shape\n",
    "    expression(Ïˆ): 100-dim expression blendshapes\n",
    "    pose(Î¸): 15-dim (global + jaw + neck + eyes)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, flame_path, n_shape=300, n_exp=100):\n",
    "        super().__init__()\n",
    "        with open(flame_path, 'rb') as f:\n",
    "            flame_data = pickle.load(f, encoding='latin1')\n",
    "\n",
    "        # Template mesh vertices\n",
    "        self.register_buffer('v_template', torch.tensor(\n",
    "            np.array(flame_data['v_template']), dtype=torch.float32))\n",
    "\n",
    "        # Shape blendshapes\n",
    "        shapedirs = np.array(flame_data['shapedirs'][:, :, :n_shape])\n",
    "        self.register_buffer('shapedirs', torch.tensor(shapedirs, dtype=torch.float32))\n",
    "\n",
    "        # Expression blendshapes\n",
    "        exprdirs = np.array(flame_data['shapedirs'][:, :, 300:300+n_exp])\n",
    "        self.register_buffer('exprdirs', torch.tensor(exprdirs, dtype=torch.float32))\n",
    "\n",
    "        # Pose blendshapes\n",
    "        posedirs = np.array(flame_data['posedirs'])\n",
    "        self.register_buffer('posedirs', torch.tensor(\n",
    "            posedirs.reshape(posedirs.shape[0] * 3, -1).T, dtype=torch.float32))\n",
    "\n",
    "        # Skinning weights\n",
    "        self.register_buffer('lbs_weights', torch.tensor(\n",
    "            np.array(flame_data['weights']), dtype=torch.float32))\n",
    "\n",
    "        # Joint regressor\n",
    "        J_regressor = np.array(flame_data['J_regressor'].todense())\n",
    "        self.register_buffer('J_regressor', torch.tensor(J_regressor, dtype=torch.float32))\n",
    "\n",
    "        # Kinematic tree\n",
    "        self.register_buffer('kintree_table', torch.tensor(\n",
    "            np.array(flame_data['kintree_table']).astype(np.int64)))\n",
    "\n",
    "        # Faces\n",
    "        self.register_buffer('faces_tensor', torch.tensor(\n",
    "            np.array(flame_data['f']).astype(np.int64)))\n",
    "\n",
    "        self.n_vertices = self.v_template.shape[0]  # 5023\n",
    "        self.n_shape = n_shape\n",
    "        self.n_exp = n_exp\n",
    "\n",
    "    def forward(self, shape_params, expression_params, pose_params):\n",
    "        \"\"\"Forward pass: params â†’ deformed vertices.\n",
    "\n",
    "        Args:\n",
    "            shape_params: (B, n_shape) identity shape\n",
    "            expression_params: (B, n_exp) expression\n",
    "            pose_params: (B, 15) pose (global + jaw + neck + eyes)\n",
    "        Returns:\n",
    "            vertices: (B, 5023, 3)\n",
    "        \"\"\"\n",
    "        batch_size = shape_params.shape[0]\n",
    "\n",
    "        # Apply shape and expression blendshapes\n",
    "        v_shaped = self.v_template.unsqueeze(0) + \\\n",
    "            torch.einsum('bl,mkl->bmk', shape_params, self.shapedirs) + \\\n",
    "            torch.einsum('bl,mkl->bmk', expression_params, self.exprdirs)\n",
    "\n",
    "        # Joint locations\n",
    "        J = torch.einsum('ji,bik->bjk', self.J_regressor, v_shaped)\n",
    "\n",
    "        # Apply LBS (simplified â€” full version uses rodrigues + kinematic chain)\n",
    "        vertices = v_shaped  # For initial stage, just blendshapes\n",
    "\n",
    "        return vertices\n",
    "\n",
    "    @property\n",
    "    def faces(self):\n",
    "        return self.faces_tensor\n",
    "\n",
    "\n",
    "print('âœ“ FLAME model defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b2235c",
   "metadata": {},
   "source": [
    "## 2. UV-Space Gaussian Avatar Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc54fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# UV-SPACE GAUSSIAN AVATAR (GAGAvatar + UHAP pattern)\n",
    "# ============================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, upsample=False):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.norm = nn.InstanceNorm2d(out_ch)\n",
    "        self.act = nn.LeakyReLU(0.2)\n",
    "        self.upsample = upsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.upsample:\n",
    "            x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        return self.act(self.norm(self.conv(x)))\n",
    "\n",
    "\n",
    "class NeutralDecoder(nn.Module):\n",
    "    \"\"\"Decodes Z_id into identity-specific bias maps (UHAP pattern).\n",
    "    Injected into Gaussian decoder at multiple scales.\"\"\"\n",
    "\n",
    "    def __init__(self, z_dim=512, num_scales=8):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(z_dim, 256 * 4 * 4)\n",
    "        channels = [256, 256, 128, 128, 64, 64, 32, 16]\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for i in range(num_scales):\n",
    "            in_ch = 256 if i == 0 else channels[i-1]\n",
    "            self.blocks.append(nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_ch, channels[i], 4, 2, 1),\n",
    "                nn.LeakyReLU(0.2)\n",
    "            ))\n",
    "\n",
    "    def forward(self, z_id):\n",
    "        x = self.fc(z_id).view(-1, 256, 4, 4)\n",
    "        bias_maps = []\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "            bias_maps.append(x)\n",
    "        return bias_maps\n",
    "\n",
    "\n",
    "class GuideMeshDecoder(nn.Module):\n",
    "    \"\"\"Predicts guide mesh vertex offsets from Z_id + Z_exp.\"\"\"\n",
    "\n",
    "    def __init__(self, z_id_dim=512, z_exp_dim=256, n_vertices=7306):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(z_id_dim + z_exp_dim, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(2048, n_vertices * 3)\n",
    "        )\n",
    "        self.n_vertices = n_vertices\n",
    "\n",
    "    def forward(self, z_id, z_exp):\n",
    "        z = torch.cat([z_id, z_exp], dim=-1)\n",
    "        offsets = self.net(z).view(-1, self.n_vertices, 3)\n",
    "        return offsets\n",
    "\n",
    "\n",
    "class GaussianAvatarDecoder(nn.Module):\n",
    "    \"\"\"Decodes Z_id + Z_exp + bias_maps â†’ UV Gaussian attribute maps.\n",
    "\n",
    "    Outputs 14-channel UV map:\n",
    "      - position offset (3)\n",
    "      - rotation quaternion (4)\n",
    "      - scale (3)\n",
    "      - opacity (1)\n",
    "      - color RGB (3)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, z_id_dim=512, z_exp_dim=256, uv_size=256):\n",
    "        super().__init__()\n",
    "        self.uv_size = uv_size\n",
    "\n",
    "        # View-independent decoder (geometry: pos, rot, scale, opacity = 11ch)\n",
    "        self.fc_vi = nn.Linear(z_id_dim + z_exp_dim, 256 * 8 * 8)\n",
    "        vi_channels = [256, 128, 128, 64, 64, 32, 16, 11]\n",
    "        self.vi_blocks = nn.ModuleList()\n",
    "        for i, out_ch in enumerate(vi_channels):\n",
    "            in_ch = 256 if i == 0 else vi_channels[i-1]\n",
    "            self.vi_blocks.append(nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_ch, out_ch, 4, 2, 1),\n",
    "                nn.LeakyReLU(0.2) if i < len(vi_channels) - 1 else nn.Identity()\n",
    "            ))\n",
    "\n",
    "        # Appearance decoder (color: RGB = 3ch, view-dependent)\n",
    "        self.fc_rgb = nn.Linear(z_id_dim + z_exp_dim + 3, 256 * 8 * 8)  # +3 for view dir\n",
    "        rgb_channels = [256, 128, 128, 64, 64, 32, 16, 3]\n",
    "        self.rgb_blocks = nn.ModuleList()\n",
    "        for i, out_ch in enumerate(rgb_channels):\n",
    "            in_ch = 256 if i == 0 else rgb_channels[i-1]\n",
    "            self.rgb_blocks.append(nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_ch, out_ch, 4, 2, 1),\n",
    "                nn.LeakyReLU(0.2) if i < len(rgb_channels) - 1 else nn.Sigmoid()\n",
    "            ))\n",
    "\n",
    "    def forward(self, z_id, z_exp, view_dir=None, bias_maps=None):\n",
    "        B = z_id.shape[0]\n",
    "        z = torch.cat([z_id, z_exp], dim=-1)\n",
    "\n",
    "        # View-independent (geometry)\n",
    "        x_vi = self.fc_vi(z).view(B, 256, 8, 8)\n",
    "        for i, block in enumerate(self.vi_blocks):\n",
    "            x_vi = block(x_vi)\n",
    "            # Inject neutral bias maps at matching scales\n",
    "            if bias_maps is not None and i < len(bias_maps):\n",
    "                bm = bias_maps[i]\n",
    "                if bm.shape[2:] == x_vi.shape[2:] and bm.shape[1] == x_vi.shape[1]:\n",
    "                    x_vi = x_vi + bm\n",
    "\n",
    "        # Crop/pad to target UV size\n",
    "        x_vi = F.interpolate(x_vi, size=(self.uv_size, self.uv_size), mode='bilinear', align_corners=False)\n",
    "\n",
    "        # View-dependent (color)\n",
    "        if view_dir is None:\n",
    "            view_dir = torch.zeros(B, 3, device=z.device)  # frontal\n",
    "        z_rgb = torch.cat([z, view_dir], dim=-1)\n",
    "        x_rgb = self.fc_rgb(z_rgb).view(B, 256, 8, 8)\n",
    "        for i, block in enumerate(self.rgb_blocks):\n",
    "            x_rgb = block(x_rgb)\n",
    "            if bias_maps is not None and i < len(bias_maps):\n",
    "                bm = bias_maps[i]\n",
    "                if bm.shape[2:] == x_rgb.shape[2:] and bm.shape[1] == x_rgb.shape[1]:\n",
    "                    x_rgb = x_rgb + bm\n",
    "\n",
    "        x_rgb = F.interpolate(x_rgb, size=(self.uv_size, self.uv_size), mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Combine: (B, 14, K, K)\n",
    "        uv_maps = torch.cat([x_vi, x_rgb], dim=1)\n",
    "        return uv_maps\n",
    "\n",
    "\n",
    "class ExpressionEncoder(nn.Module):\n",
    "    \"\"\"VAE encoder: UV texture/geometry difference maps â†’ Z_exp (256-dim).\"\"\"\n",
    "\n",
    "    def __init__(self, z_dim=256):\n",
    "        super().__init__()\n",
    "        channels = [32, 32, 64, 64, 128, 128, 256, 256]\n",
    "        layers = []\n",
    "        in_ch = 6  # 3ch texture diff + 3ch geometry diff\n",
    "        for out_ch in channels:\n",
    "            layers.extend([\n",
    "                nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.AvgPool2d(2)\n",
    "            ])\n",
    "            in_ch = out_ch\n",
    "        self.encoder = nn.Sequential(*layers)  # 512â†’2Ã—2\n",
    "        self.fc_mu = nn.Linear(256 * 2 * 2, z_dim)\n",
    "        self.fc_logvar = nn.Linear(256 * 2 * 2, z_dim)\n",
    "\n",
    "    def forward(self, delta_tex, delta_geo):\n",
    "        x = torch.cat([delta_tex, delta_geo], dim=1)  # (B, 6, 512, 512)\n",
    "        h = self.encoder(x).flatten(1)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        # Reparameterization trick\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z_exp = mu + eps * std\n",
    "        return z_exp, mu, logvar\n",
    "\n",
    "\n",
    "class QuantumHeadModel(nn.Module):\n",
    "    \"\"\"Full QuantumHead model â€” combines all decoders.\"\"\"\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        super().__init__()\n",
    "        z_id = config.get('z_id_dim', 512) if config else 512\n",
    "        z_exp = config.get('z_exp_dim', 256) if config else 256\n",
    "        uv = config.get('uv_size', 256) if config else 256\n",
    "        n_verts = config.get('guide_vertices', 7306) if config else 7306\n",
    "\n",
    "        self.expression_encoder = ExpressionEncoder(z_dim=z_exp)\n",
    "        self.neutral_decoder = NeutralDecoder(z_dim=z_id)\n",
    "        self.guide_mesh_decoder = GuideMeshDecoder(z_id, z_exp, n_verts)\n",
    "        self.gaussian_decoder = GaussianAvatarDecoder(z_id, z_exp, uv)\n",
    "\n",
    "    def forward(self, z_id, z_exp, view_dir=None):\n",
    "        \"\"\"Full forward: latents â†’ UV Gaussian attribute maps + guide mesh.\"\"\"\n",
    "        bias_maps = self.neutral_decoder(z_id)\n",
    "        guide_offsets = self.guide_mesh_decoder(z_id, z_exp)\n",
    "        uv_maps = self.gaussian_decoder(z_id, z_exp, view_dir, bias_maps)\n",
    "        return uv_maps, guide_offsets\n",
    "\n",
    "    def encode_expression(self, delta_tex, delta_geo):\n",
    "        return self.expression_encoder(delta_tex, delta_geo)\n",
    "\n",
    "\n",
    "# Quick test\n",
    "model = QuantumHeadModel()\n",
    "z_id = torch.randn(2, 512)\n",
    "z_exp = torch.randn(2, 256)\n",
    "uv_maps, guide = model(z_id, z_exp)\n",
    "print(f'UV maps: {uv_maps.shape}')     # (2, 14, 256, 256)\n",
    "print(f'Guide mesh: {guide.shape}')     # (2, 7306, 3)\n",
    "params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Total params: {params/1e6:.1f}M')\n",
    "del model, z_id, z_exp, uv_maps, guide\n",
    "print('âœ“ QuantumHead model verified')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643dafb7",
   "metadata": {},
   "source": [
    "## 3. Audio â†’ FLAME Motion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12192491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# AUDIO-TO-FLAME TRANSFORMER (GaussianHeadTalk pattern)\n",
    "# ============================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class PeriodicPositionalEncoding(nn.Module):\n",
    "    \"\"\"Periodic positional encoding for audio sequence.\"\"\"\n",
    "    def __init__(self, d_model, max_len=5000, period=25):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # Add periodic component\n",
    "        pe[:, 0::2] += torch.sin(position * 2 * math.pi / period)\n",
    "        pe[:, 1::2] += torch.cos(position * 2 * math.pi / period)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class StyleEncoder(nn.Module):\n",
    "    \"\"\"Encodes identity-specific speaking style from template mesh.\"\"\"\n",
    "    def __init__(self, n_vertices=5023, d_model=512):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_vertices * 3, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, template_mesh):\n",
    "        return self.net(template_mesh.flatten(1))\n",
    "\n",
    "\n",
    "class Audio2FLAMETransformer(nn.Module):\n",
    "    \"\"\"Wav2Vec2 â†’ Transformer decoder â†’ FLAME expression params.\n",
    "\n",
    "    Predicts FLAME params directly (not vertices) for stability.\n",
    "    Output: 53 params (50 expression + 3 jaw pose)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model=512, nhead=8, num_layers=6, n_flame_params=53):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Audio feature projection (Wav2Vec2 output is 1024-dim)\n",
    "        self.audio_proj = nn.Linear(1024, d_model)\n",
    "\n",
    "        # Positional encoding\n",
    "        self.pos_enc = PeriodicPositionalEncoding(d_model, period=25)  # 25fps\n",
    "\n",
    "        # Transformer decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model, nhead=nhead,\n",
    "            dim_feedforward=2048, dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            decoder_layer, num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        # Style embedding\n",
    "        self.style_encoder = StyleEncoder(d_model=d_model)\n",
    "\n",
    "        # Output heads\n",
    "        self.flame_head = nn.Sequential(\n",
    "            nn.Linear(d_model, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_flame_params)\n",
    "        )\n",
    "\n",
    "    def forward(self, audio_features, style_embedding, causal_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio_features: (B, T, 1024) from Wav2Vec2\n",
    "            style_embedding: (B, d_model) from StyleEncoder\n",
    "            causal_mask: optional causal mask for autoregressive\n",
    "        Returns:\n",
    "            flame_params: (B, T, 53) â€” 50 expression + 3 jaw\n",
    "        \"\"\"\n",
    "        B, T, _ = audio_features.shape\n",
    "\n",
    "        # Project audio to d_model\n",
    "        audio = self.audio_proj(audio_features)  # (B, T, 512)\n",
    "        audio = self.pos_enc(audio)\n",
    "\n",
    "        # Style as initial query\n",
    "        style = style_embedding.unsqueeze(1).expand(-1, T, -1)  # (B, T, 512)\n",
    "\n",
    "        # Generate causal mask\n",
    "        if causal_mask is None:\n",
    "            causal_mask = nn.Transformer.generate_square_subsequent_mask(\n",
    "                T, device=audio.device)\n",
    "\n",
    "        # Decode\n",
    "        output = self.transformer_decoder(\n",
    "            tgt=style,\n",
    "            memory=audio,\n",
    "            tgt_mask=causal_mask\n",
    "        )\n",
    "\n",
    "        # Predict FLAME params per frame\n",
    "        flame_params = self.flame_head(output)  # (B, T, 53)\n",
    "        return flame_params\n",
    "\n",
    "\n",
    "# Quick test\n",
    "a2f = Audio2FLAMETransformer()\n",
    "audio_feat = torch.randn(2, 100, 1024)  # 100 frames of wav2vec2\n",
    "style = torch.randn(2, 512)\n",
    "params = a2f(audio_feat, style)\n",
    "print(f'FLAME params: {params.shape}')  # (2, 100, 53)\n",
    "n_params = sum(p.numel() for p in a2f.parameters())\n",
    "print(f'Audio2FLAME params: {n_params/1e6:.1f}M')\n",
    "del a2f, audio_feat, style, params\n",
    "print('âœ“ Audio2FLAME transformer verified')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6219d374",
   "metadata": {},
   "source": [
    "## 4. Expression Diffusion Model (UHAP pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a0bf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXPRESSION DIFFUSION MODEL (UHAP pattern â€” Audio â†’ Z_exp)\n",
    "# Maps Wav2Vec2 audio + lip vertices â†’ expression latent codes\n",
    "# via DDPM for richer expressions beyond FLAME params\n",
    "# ============================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class FiLMLayer(nn.Module):\n",
    "    \"\"\"Feature-wise Linear Modulation for timestep conditioning.\"\"\"\n",
    "    def __init__(self, d_model, d_cond):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Linear(d_cond, d_model)\n",
    "        self.beta = nn.Linear(d_cond, d_model)\n",
    "\n",
    "    def forward(self, x, cond):\n",
    "        gamma = self.gamma(cond).unsqueeze(1)  # (B, 1, D)\n",
    "        beta = self.beta(cond).unsqueeze(1)\n",
    "        return gamma * x + beta\n",
    "\n",
    "\n",
    "class ExpressionDiffusionTransformer(nn.Module):\n",
    "    \"\"\"DDPM backbone: denoises Z_exp conditioned on audio + lip vertices.\n",
    "\n",
    "    Based on UHAP's design:\n",
    "    - Self-attention on noisy expression codes\n",
    "    - Cross-attention to audio features + lip vertices\n",
    "    - FiLM layers for timestep embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, z_dim=256, d_model=512, nhead=8, num_layers=6):\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        # Input projections\n",
    "        self.z_proj = nn.Linear(z_dim, d_model)\n",
    "        self.audio_proj = nn.Linear(1024, d_model)  # Wav2Vec2\n",
    "        self.lip_proj = nn.Linear(338 * 3, d_model)  # 338 lip vertices Ã— 3\n",
    "\n",
    "        # Timestep embedding\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(256, d_model),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(d_model, d_model)\n",
    "        )\n",
    "\n",
    "        # Transformer layers with FiLM\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.film_layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.layers.append(nn.TransformerDecoderLayer(\n",
    "                d_model=d_model, nhead=nhead,\n",
    "                dim_feedforward=2048, dropout=0.1,\n",
    "                batch_first=True\n",
    "            ))\n",
    "            self.film_layers.append(FiLMLayer(d_model, d_model))\n",
    "\n",
    "        # Output projection\n",
    "        self.out_proj = nn.Linear(d_model, z_dim)\n",
    "\n",
    "    def get_timestep_embedding(self, timesteps, dim=256):\n",
    "        half = dim // 2\n",
    "        freqs = torch.exp(-torch.arange(half, device=timesteps.device).float() *\n",
    "                         (torch.log(torch.tensor(10000.0)) / half))\n",
    "        args = timesteps.float().unsqueeze(1) * freqs.unsqueeze(0)\n",
    "        return torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "\n",
    "    def forward(self, z_noisy, timestep, audio_features, lip_vertices):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            z_noisy: (B, T, z_dim) noisy expression codes\n",
    "            timestep: (B,) diffusion timestep\n",
    "            audio_features: (B, T, 1024) from Wav2Vec2\n",
    "            lip_vertices: (B, T, 338*3) predicted lip vertices\n",
    "        Returns:\n",
    "            noise_pred: (B, T, z_dim) predicted noise\n",
    "        \"\"\"\n",
    "        # Embed inputs\n",
    "        z = self.z_proj(z_noisy)\n",
    "        audio = self.audio_proj(audio_features)\n",
    "        lip = self.lip_proj(lip_vertices)\n",
    "\n",
    "        # Conditioning: concat audio + lip\n",
    "        memory = audio + lip\n",
    "\n",
    "        # Timestep\n",
    "        t_emb = self.get_timestep_embedding(timestep)\n",
    "        t_emb = self.time_embed(t_emb)\n",
    "\n",
    "        # Transformer + FiLM\n",
    "        h = z\n",
    "        for layer, film in zip(self.layers, self.film_layers):\n",
    "            h = layer(h, memory)\n",
    "            h = film(h, t_emb)\n",
    "\n",
    "        return self.out_proj(h)\n",
    "\n",
    "\n",
    "# Quick test\n",
    "diff = ExpressionDiffusionTransformer()\n",
    "z = torch.randn(2, 50, 256)\n",
    "t = torch.randint(0, 500, (2,))\n",
    "a = torch.randn(2, 50, 1024)\n",
    "l = torch.randn(2, 50, 338*3)\n",
    "out = diff(z, t, a, l)\n",
    "print(f'Noise pred: {out.shape}')  # (2, 50, 256)\n",
    "n = sum(p.numel() for p in diff.parameters())\n",
    "print(f'Diffusion params: {n/1e6:.1f}M')\n",
    "del diff, z, t, a, l, out\n",
    "print('âœ“ Expression diffusion model verified')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a729a393",
   "metadata": {},
   "source": [
    "## 5. Gaussian Splatting Renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7038ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3D GAUSSIAN SPLATTING RENDERER\n",
    "# Converts UV attribute maps â†’ 3D Gaussians â†’ rendered image\n",
    "# ============================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class GaussianRenderer(nn.Module):\n",
    "    \"\"\"Renders 3D Gaussians from UV attribute maps.\n",
    "\n",
    "    UV maps â†’ sample Gaussians â†’ rasterize â†’ image.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, uv_size=256, image_size=512):\n",
    "        super().__init__()\n",
    "        self.uv_size = uv_size\n",
    "        self.image_size = image_size\n",
    "\n",
    "        # UV mask (which texels are valid â€” matches FLAME topology)\n",
    "        # In full implementation, this comes from FLAME UV layout\n",
    "        self.register_buffer('uv_mask', torch.ones(uv_size, uv_size, dtype=torch.bool))\n",
    "\n",
    "        # Sampling grid for Gaussians\n",
    "        grid_y, grid_x = torch.meshgrid(\n",
    "            torch.linspace(-1, 1, uv_size),\n",
    "            torch.linspace(-1, 1, uv_size),\n",
    "            indexing='ij'\n",
    "        )\n",
    "        self.register_buffer('sample_grid', torch.stack([grid_x, grid_y], dim=-1))\n",
    "\n",
    "    def uv_to_gaussians(self, uv_maps, position_map):\n",
    "        \"\"\"Convert UV attribute maps to Gaussian parameters.\n",
    "\n",
    "        Args:\n",
    "            uv_maps: (B, 14, K, K) â€” pos_offset(3), rot(4), scale(3), opacity(1), color(3)\n",
    "            position_map: (B, 3, K, K) â€” base 3D positions from FLAME mesh\n",
    "        Returns:\n",
    "            dict of Gaussian parameters\n",
    "        \"\"\"\n",
    "        B = uv_maps.shape[0]\n",
    "\n",
    "        # Split channels\n",
    "        pos_offset = uv_maps[:, 0:3]   # (B, 3, K, K)\n",
    "        rotation = uv_maps[:, 3:7]     # (B, 4, K, K)\n",
    "        scale = uv_maps[:, 7:10]       # (B, 3, K, K)\n",
    "        opacity = uv_maps[:, 10:11]    # (B, 1, K, K)\n",
    "        color = uv_maps[:, 11:14]      # (B, 3, K, K)\n",
    "\n",
    "        # Final positions = base + offset\n",
    "        positions = position_map + pos_offset\n",
    "\n",
    "        # Flatten UV to point cloud: (B, N, C)\n",
    "        mask = self.uv_mask.flatten()  # (K*K,)\n",
    "\n",
    "        def flatten_uv(t):\n",
    "            B, C, H, W = t.shape\n",
    "            return t.reshape(B, C, H*W).permute(0, 2, 1)[:, mask]  # (B, N_valid, C)\n",
    "\n",
    "        return {\n",
    "            'positions': flatten_uv(positions),\n",
    "            'rotations': F.normalize(flatten_uv(rotation), dim=-1),\n",
    "            'scales': torch.exp(flatten_uv(scale)),\n",
    "            'opacities': torch.sigmoid(flatten_uv(opacity)),\n",
    "            'colors': flatten_uv(color),\n",
    "        }\n",
    "\n",
    "    def render(self, gaussians, camera):\n",
    "        \"\"\"Render Gaussians to image using gsplat.\n",
    "\n",
    "        Falls back to neural rendering if gsplat unavailable.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import gsplat\n",
    "            # Full gsplat rasterization\n",
    "            rendered = gsplat.rasterization(\n",
    "                means=gaussians['positions'][0],\n",
    "                quats=gaussians['rotations'][0],\n",
    "                scales=gaussians['scales'][0],\n",
    "                opacities=gaussians['opacities'][0].squeeze(-1),\n",
    "                colors=gaussians['colors'][0],\n",
    "                viewmats=camera['viewmat'].unsqueeze(0),\n",
    "                Ks=camera['K'].unsqueeze(0),\n",
    "                width=self.image_size,\n",
    "                height=self.image_size,\n",
    "            )\n",
    "            return rendered[0]  # (H, W, 3)\n",
    "        except (ImportError, Exception):\n",
    "            # Fallback: simple neural renderer for training\n",
    "            return self._neural_render(gaussians)\n",
    "\n",
    "    def _neural_render(self, gaussians):\n",
    "        \"\"\"Simple differentiable neural renderer fallback.\"\"\"\n",
    "        B = gaussians['positions'].shape[0]\n",
    "        # Project to 2D (simplified orthographic)\n",
    "        pos_2d = gaussians['positions'][:, :, :2]  # (B, N, 2)\n",
    "        colors = gaussians['colors']  # (B, N, 3)\n",
    "        opacities = gaussians['opacities']  # (B, N, 1)\n",
    "\n",
    "        # Splatting via scatter (simplified)\n",
    "        H = W = self.image_size\n",
    "        img = torch.zeros(B, 3, H, W, device=pos_2d.device)\n",
    "\n",
    "        # Convert positions to pixel coords\n",
    "        px = ((pos_2d[:, :, 0] + 1) * 0.5 * W).long().clamp(0, W-1)\n",
    "        py = ((pos_2d[:, :, 1] + 1) * 0.5 * H).long().clamp(0, H-1)\n",
    "\n",
    "        for b in range(B):\n",
    "            for c in range(3):\n",
    "                img[b, c].index_put_(\n",
    "                    (py[b], px[b]),\n",
    "                    colors[b, :, c] * opacities[b, :, 0],\n",
    "                    accumulate=True\n",
    "                )\n",
    "\n",
    "        return img.clamp(0, 1)\n",
    "\n",
    "\n",
    "print('âœ“ Gaussian renderer defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63583ca9",
   "metadata": {},
   "source": [
    "## 6. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10a1ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAINING LOOP\n",
    "# ============================================================\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train_quantumhead(\n",
    "    model,\n",
    "    audio_model,\n",
    "    renderer,\n",
    "    dataloader,\n",
    "    num_iterations=50000,\n",
    "    lr=1e-4,\n",
    "    checkpoint_dir='/content/quantumhead_output/checkpoints',\n",
    "    save_every=5000,\n",
    "):\n",
    "    \"\"\"Main training loop for QuantumHead.\"\"\"\n",
    "\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "    model = model.to(device)\n",
    "    audio_model = audio_model.to(device)\n",
    "    renderer = renderer.to(device)\n",
    "\n",
    "    # Optimizers\n",
    "    opt_model = Adam(model.parameters(), lr=lr)\n",
    "    opt_audio = Adam(audio_model.parameters(), lr=lr)\n",
    "\n",
    "    # Loss weights (from UHAP paper)\n",
    "    w_rec = 1.0       # Image reconstruction (L1 + SSIM)\n",
    "    w_neut = 0.5      # Neutral scan reconstruction\n",
    "    w_kl = 0.001      # KL divergence\n",
    "    w_geo = 1.0       # Guide mesh geometry\n",
    "    w_perc = 0.1      # Perceptual (LPIPS)\n",
    "\n",
    "    try:\n",
    "        import lpips\n",
    "        lpips_fn = lpips.LPIPS(net='vgg').to(device)\n",
    "    except ImportError:\n",
    "        lpips_fn = None\n",
    "\n",
    "    model.train()\n",
    "    audio_model.train()\n",
    "\n",
    "    step = 0\n",
    "    pbar = tqdm(total=num_iterations, desc='Training')\n",
    "\n",
    "    while step < num_iterations:\n",
    "        for batch in dataloader:\n",
    "            if step >= num_iterations:\n",
    "                break\n",
    "\n",
    "            # Unpack batch\n",
    "            images = batch['image'].to(device)           # (B, 3, H, W)\n",
    "            flame_shape = batch['shape'].to(device)      # (B, 300)\n",
    "            flame_exp = batch['expression'].to(device)   # (B, 100)\n",
    "            flame_pose = batch['pose'].to(device)        # (B, 15)\n",
    "            delta_tex = batch.get('delta_tex', torch.zeros(images.shape[0], 3, 512, 512)).to(device)\n",
    "            delta_geo = batch.get('delta_geo', torch.zeros(images.shape[0], 3, 512, 512)).to(device)\n",
    "\n",
    "            # --- Forward ---\n",
    "            # Encode expression\n",
    "            z_exp, mu, logvar = model.encode_expression(delta_tex, delta_geo)\n",
    "\n",
    "            # Identity code (learnable per subject)\n",
    "            z_id = torch.randn(images.shape[0], 512, device=device)  # TODO: per-subject optimization\n",
    "\n",
    "            # Decode\n",
    "            uv_maps, guide_offsets = model(z_id, z_exp)\n",
    "\n",
    "            # Create position map from FLAME (simplified)\n",
    "            pos_map = torch.zeros_like(uv_maps[:, :3])  # TODO: from FLAME model\n",
    "\n",
    "            # Render\n",
    "            gaussians = renderer.uv_to_gaussians(uv_maps, pos_map)\n",
    "            rendered = renderer._neural_render(gaussians)\n",
    "\n",
    "            # --- Losses ---\n",
    "            # Reconstruction\n",
    "            target = F.interpolate(images, size=rendered.shape[2:], mode='bilinear', align_corners=False)\n",
    "            loss_rec = F.l1_loss(rendered, target)\n",
    "\n",
    "            # KL divergence\n",
    "            loss_kl = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "            # Perceptual\n",
    "            loss_perc = torch.tensor(0.0, device=device)\n",
    "            if lpips_fn is not None:\n",
    "                loss_perc = lpips_fn(rendered * 2 - 1, target * 2 - 1).mean()\n",
    "\n",
    "            # Total\n",
    "            loss = w_rec * loss_rec + w_kl * loss_kl + w_perc * loss_perc\n",
    "\n",
    "            # --- Backward ---\n",
    "            opt_model.zero_grad()\n",
    "            opt_audio.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt_model.step()\n",
    "            opt_audio.step()\n",
    "\n",
    "            # Log\n",
    "            if step % 100 == 0:\n",
    "                pbar.set_postfix({\n",
    "                    'loss': f'{loss.item():.4f}',\n",
    "                    'rec': f'{loss_rec.item():.4f}',\n",
    "                    'kl': f'{loss_kl.item():.4f}',\n",
    "                })\n",
    "\n",
    "            # Checkpoint\n",
    "            if step > 0 and step % save_every == 0:\n",
    "                ckpt = {\n",
    "                    'step': step,\n",
    "                    'model': model.state_dict(),\n",
    "                    'audio_model': audio_model.state_dict(),\n",
    "                    'opt_model': opt_model.state_dict(),\n",
    "                    'opt_audio': opt_audio.state_dict(),\n",
    "                }\n",
    "                path = os.path.join(checkpoint_dir, f'ckpt_{step:06d}.pt')\n",
    "                torch.save(ckpt, path)\n",
    "                print(f'\\nâœ“ Saved checkpoint: {path}')\n",
    "\n",
    "            step += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    # Final save\n",
    "    final_path = os.path.join(checkpoint_dir, 'final.pt')\n",
    "    torch.save({\n",
    "        'step': step,\n",
    "        'model': model.state_dict(),\n",
    "        'audio_model': audio_model.state_dict(),\n",
    "    }, final_path)\n",
    "    print(f'âœ“ Final model saved: {final_path}')\n",
    "    return final_path\n",
    "\n",
    "\n",
    "print('âœ“ Training loop defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f80abd4",
   "metadata": {},
   "source": [
    "## 7. Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc540489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATASET â€” Loads face video frames with FLAME annotations\n",
    "# Supports: VFHQ, HDTF, VOCASET, or custom selfies\n",
    "# ============================================================\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class FaceVideoDataset(Dataset):\n",
    "    \"\"\"Dataset of face video frames with FLAME parameters.\n",
    "\n",
    "    Directory structure:\n",
    "      data_root/\n",
    "        subject_001/\n",
    "          frames/        # extracted video frames (512x512)\n",
    "          flame/         # FLAME params per frame (.npz)\n",
    "          audio/         # audio features (.npy)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_root, image_size=512, split='train'):\n",
    "        self.data_root = Path(data_root)\n",
    "        self.image_size = image_size\n",
    "        self.samples = []\n",
    "\n",
    "        # Scan for frame/FLAME pairs\n",
    "        for subject_dir in sorted(self.data_root.iterdir()):\n",
    "            if not subject_dir.is_dir():\n",
    "                continue\n",
    "            frames_dir = subject_dir / 'frames'\n",
    "            flame_dir = subject_dir / 'flame'\n",
    "            if not frames_dir.exists():\n",
    "                continue\n",
    "\n",
    "            for frame_path in sorted(frames_dir.glob('*.png')):\n",
    "                flame_path = flame_dir / frame_path.with_suffix('.npz').name\n",
    "                self.samples.append({\n",
    "                    'frame': str(frame_path),\n",
    "                    'flame': str(flame_path) if flame_path.exists() else None,\n",
    "                    'subject': subject_dir.name,\n",
    "                })\n",
    "\n",
    "        print(f'âœ“ Dataset: {len(self.samples)} frames from {len(set(s[\"subject\"] for s in self.samples))} subjects')\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.samples), 1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if len(self.samples) == 0:\n",
    "            # Return dummy data for testing\n",
    "            return {\n",
    "                'image': torch.randn(3, self.image_size, self.image_size),\n",
    "                'shape': torch.zeros(300),\n",
    "                'expression': torch.zeros(100),\n",
    "                'pose': torch.zeros(15),\n",
    "                'delta_tex': torch.zeros(3, 512, 512),\n",
    "                'delta_geo': torch.zeros(3, 512, 512),\n",
    "            }\n",
    "\n",
    "        sample = self.samples[idx % len(self.samples)]\n",
    "\n",
    "        # Load image\n",
    "        img = cv2.imread(sample['frame'])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (self.image_size, self.image_size))\n",
    "        img = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0\n",
    "\n",
    "        # Load FLAME params\n",
    "        if sample['flame'] and os.path.exists(sample['flame']):\n",
    "            flame = np.load(sample['flame'])\n",
    "            shape = torch.from_numpy(flame.get('shape', np.zeros(300))).float()\n",
    "            expression = torch.from_numpy(flame.get('expression', np.zeros(100))).float()\n",
    "            pose = torch.from_numpy(flame.get('pose', np.zeros(15))).float()\n",
    "        else:\n",
    "            shape = torch.zeros(300)\n",
    "            expression = torch.zeros(100)\n",
    "            pose = torch.zeros(15)\n",
    "\n",
    "        return {\n",
    "            'image': img,\n",
    "            'shape': shape,\n",
    "            'expression': expression,\n",
    "            'pose': pose,\n",
    "            'delta_tex': torch.zeros(3, 512, 512),\n",
    "            'delta_geo': torch.zeros(3, 512, 512),\n",
    "        }\n",
    "\n",
    "\n",
    "print('âœ“ Dataset class defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2515dd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DOWNLOAD & PREPARE HDTF DATASET (audio-visual talking heads)\n",
    "# ============================================================\n",
    "import os\n",
    "\n",
    "DATA_ROOT = '/content/quantumhead_data'\n",
    "os.makedirs(DATA_ROOT, exist_ok=True)\n",
    "\n",
    "print('=== Dataset Options ===')\n",
    "print('1. HDTF (High-Definition Talking Face) â€” ~15.8 hours, 720p+')\n",
    "print('   git clone https://github.com/MRzzm/HDTF.git')\n",
    "print()\n",
    "print('2. VFHQ (Video Face Super-Resolution HQ) â€” large-scale face videos')\n",
    "print('   Requires application at: https://liangbinxie.github.io/projects/vfhq/')\n",
    "print()\n",
    "print('3. VOCASET (VOice Controlled Avatars) â€” audio + 3D tracked FLAME')\n",
    "print('   https://voca.is.tue.mpg.de/')\n",
    "print()\n",
    "print('4. Custom selfies from spike2 (your 12 selfies)')\n",
    "print('   Will auto-download from spike2 server')\n",
    "print()\n",
    "print('For quick start, we\\'ll use your selfies + HDTF samples.')\n",
    "print('Upload data to:', DATA_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ce2951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PULL SELFIES FROM SPIKE2\n",
    "# ============================================================\n",
    "import os\n",
    "import requests\n",
    "\n",
    "SELFIE_DIR = os.path.join(DATA_ROOT, 'riley_selfies/frames')\n",
    "os.makedirs(SELFIE_DIR, exist_ok=True)\n",
    "\n",
    "# Pull selfies from spike2 via the API\n",
    "SPIKE2_URL = f'https://{SPIKE2_HOST}'\n",
    "\n",
    "try:\n",
    "    # List available selfie sets\n",
    "    resp = requests.get(f'{SPIKE2_URL}/avatar/models', timeout=10, verify=False)\n",
    "    if resp.ok:\n",
    "        models = resp.json()\n",
    "        print(f'Found {len(models)} avatar models on spike2')\n",
    "        for m in models:\n",
    "            print(f'  - {m}')\n",
    "    else:\n",
    "        print(f'Could not reach spike2 API: {resp.status_code}')\n",
    "except Exception as e:\n",
    "    print(f'spike2 not reachable: {e}')\n",
    "    print('Will use dummy data for architecture testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ae8eec",
   "metadata": {},
   "source": [
    "## 8. FLAME Fitting with DECA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b73260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DECA-BASED FLAME FITTING\n",
    "# Single image â†’ FLAME shape, expression, pose params\n",
    "# ============================================================\n",
    "import sys\n",
    "sys.path.insert(0, '/content/repos/DECA')\n",
    "\n",
    "\n",
    "def fit_flame_to_image(image_path, deca_model=None):\n",
    "    \"\"\"Fit FLAME parameters to a single image using DECA.\n",
    "\n",
    "    Returns:\n",
    "        dict with 'shape' (300,), 'expression' (100,), 'pose' (15,)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from decalib.deca import DECA\n",
    "        from decalib.utils.config import cfg as deca_cfg\n",
    "\n",
    "        if deca_model is None:\n",
    "            deca_cfg.model.use_tex = False\n",
    "            deca_model = DECA(config=deca_cfg, device='cuda')\n",
    "\n",
    "        img = cv2.imread(image_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        img_tensor = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0\n",
    "        img_tensor = img_tensor.unsqueeze(0).to('cuda')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            codedict = deca_model.encode(img_tensor)\n",
    "\n",
    "        return {\n",
    "            'shape': codedict['shape'][0].cpu().numpy(),\n",
    "            'expression': codedict['exp'][0].cpu().numpy(),\n",
    "            'pose': codedict['pose'][0].cpu().numpy(),\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f'DECA fitting failed: {e}')\n",
    "        print('Using face-alignment fallback...')\n",
    "        # Fallback: use face-alignment for landmarks â†’ approximate FLAME\n",
    "        return {\n",
    "            'shape': np.zeros(300),\n",
    "            'expression': np.zeros(100),\n",
    "            'pose': np.zeros(15),\n",
    "        }\n",
    "\n",
    "\n",
    "def batch_fit_flame(image_dir, output_dir):\n",
    "    \"\"\"Fit FLAME params for all images in a directory.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    images = sorted(Path(image_dir).glob('*.png')) + sorted(Path(image_dir).glob('*.jpg'))\n",
    "\n",
    "    print(f'Fitting FLAME to {len(images)} images...')\n",
    "    for img_path in tqdm(images):\n",
    "        params = fit_flame_to_image(str(img_path))\n",
    "        out_path = Path(output_dir) / img_path.with_suffix('.npz').name\n",
    "        np.savez(str(out_path), **params)\n",
    "\n",
    "    print(f'âœ“ Saved FLAME params to {output_dir}')\n",
    "\n",
    "\n",
    "print('âœ“ FLAME fitting pipeline defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c1c0df",
   "metadata": {},
   "source": [
    "## 9. Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac324f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INITIALIZE & RUN TRAINING\n",
    "# ============================================================\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Config\n",
    "config = {\n",
    "    'z_id_dim': IDENTITY_DIM,       # 512\n",
    "    'z_exp_dim': EXPRESSION_DIM,     # 256\n",
    "    'uv_size': UV_MAP_SIZE,          # 256\n",
    "    'guide_vertices': GUIDE_MESH_VERTICES,  # 7306\n",
    "}\n",
    "\n",
    "# Initialize models\n",
    "model = QuantumHeadModel(config)\n",
    "audio_model = Audio2FLAMETransformer()\n",
    "renderer = GaussianRenderer(uv_size=UV_MAP_SIZE, image_size=512)\n",
    "\n",
    "# Dataset\n",
    "dataset = FaceVideoDataset(DATA_ROOT, image_size=512)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "# Print model sizes\n",
    "def count_params(m):\n",
    "    return sum(p.numel() for p in m.parameters()) / 1e6\n",
    "\n",
    "print(f'QuantumHead model:  {count_params(model):.1f}M params')\n",
    "print(f'Audio2FLAME model:  {count_params(audio_model):.1f}M params')\n",
    "print(f'Dataset size:       {len(dataset)} samples')\n",
    "print(f'Batch size:         {BATCH_SIZE}')\n",
    "print(f'Iterations:         {NUM_ITERATIONS}')\n",
    "print(f'\\nStarting training...')\n",
    "\n",
    "# Train\n",
    "final_path = train_quantumhead(\n",
    "    model=model,\n",
    "    audio_model=audio_model,\n",
    "    renderer=renderer,\n",
    "    dataloader=dataloader,\n",
    "    num_iterations=NUM_ITERATIONS,\n",
    "    lr=LEARNING_RATE,\n",
    "    checkpoint_dir=CHECKPOINT_DIR,\n",
    "    save_every=5000,\n",
    ")\n",
    "\n",
    "print(f'\\nâœ… Training complete! Final model: {final_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df593bb",
   "metadata": {},
   "source": [
    "## 10. Push Weights to spike2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade0dbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PUSH TRAINED WEIGHTS TO SPIKE2\n",
    "# spike2 serves inference â€” Colab just trains\n",
    "# ============================================================\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "def push_to_spike2(checkpoint_path, spike2_host, weights_dir):\n",
    "    \"\"\"Push trained weights to spike2 via SCP or HTTP upload.\"\"\"\n",
    "\n",
    "    # Method 1: Direct upload via spike2 API\n",
    "    print(f'Uploading {checkpoint_path} to spike2...')\n",
    "\n",
    "    try:\n",
    "        import requests\n",
    "        url = f'https://{spike2_host}/quantumhead/upload-weights'\n",
    "\n",
    "        file_size = os.path.getsize(checkpoint_path) / 1e6\n",
    "        print(f'  File size: {file_size:.1f} MB')\n",
    "\n",
    "        with open(checkpoint_path, 'rb') as f:\n",
    "            resp = requests.post(\n",
    "                url,\n",
    "                files={'weights': (os.path.basename(checkpoint_path), f)},\n",
    "                timeout=300,\n",
    "                verify=False,\n",
    "            )\n",
    "\n",
    "        if resp.ok:\n",
    "            result = resp.json()\n",
    "            print(f'  âœ“ Uploaded to spike2: {result}')\n",
    "            return True\n",
    "        else:\n",
    "            print(f'  âœ— Upload failed: {resp.status_code} {resp.text}')\n",
    "    except Exception as e:\n",
    "        print(f'  API upload failed: {e}')\n",
    "\n",
    "    # Method 2: SCP via SSH\n",
    "    print('  Trying SCP fallback...')\n",
    "    try:\n",
    "        import paramiko\n",
    "        from scp import SCPClient\n",
    "\n",
    "        ssh = paramiko.SSHClient()\n",
    "        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "        ssh.connect(SPIKE2_SSH)  # Uses SSH config\n",
    "\n",
    "        with SCPClient(ssh.get_transport()) as scp_client:\n",
    "            remote_path = os.path.join(weights_dir, os.path.basename(checkpoint_path))\n",
    "            scp_client.put(checkpoint_path, remote_path)\n",
    "            print(f'  âœ“ SCP\\'d to spike2:{remote_path}')\n",
    "\n",
    "        ssh.close()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f'  SCP failed: {e}')\n",
    "\n",
    "    # Method 3: Save to Google Drive (manual transfer)\n",
    "    print('  Saving to Google Drive as fallback...')\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        drive_path = '/content/drive/MyDrive/quantumhead_weights/'\n",
    "        os.makedirs(drive_path, exist_ok=True)\n",
    "        import shutil\n",
    "        shutil.copy2(checkpoint_path, drive_path)\n",
    "        print(f'  âœ“ Saved to Drive: {drive_path}')\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f'  Drive save failed: {e}')\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "# Push the final checkpoint\n",
    "if os.path.exists(final_path):\n",
    "    success = push_to_spike2(final_path, SPIKE2_HOST, SPIKE2_WEIGHTS_DIR)\n",
    "    if success:\n",
    "        print('\\nâœ… Weights pushed to spike2! Inference server can now load them.')\n",
    "    else:\n",
    "        print('\\nâš ï¸  Auto-push failed. Download the checkpoint manually:')\n",
    "        print(f'    {final_path}')\n",
    "        print(f'    Then SCP to spike2: scp {final_path} spike2:{SPIKE2_WEIGHTS_DIR}/')\n",
    "else:\n",
    "    print('No checkpoint found. Run training first.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd213bc1",
   "metadata": {},
   "source": [
    "## 11. Export for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8bc297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXPORT MODEL FOR INFERENCE\n",
    "# Creates a self-contained inference package for spike2\n",
    "# ============================================================\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def export_inference_package(model, audio_model, config, output_dir):\n",
    "    \"\"\"Export everything spike2 needs for inference.\"\"\"\n",
    "\n",
    "    pkg_dir = os.path.join(output_dir, 'inference_package')\n",
    "    os.makedirs(pkg_dir, exist_ok=True)\n",
    "\n",
    "    # 1. Model weights\n",
    "    torch.save(model.state_dict(), os.path.join(pkg_dir, 'quantumhead.pt'))\n",
    "    torch.save(audio_model.state_dict(), os.path.join(pkg_dir, 'audio2flame.pt'))\n",
    "\n",
    "    # 2. Config\n",
    "    with open(os.path.join(pkg_dir, 'config.json'), 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "    # 3. Model architecture info for loading\n",
    "    arch_info = {\n",
    "        'quantumhead': {\n",
    "            'class': 'QuantumHeadModel',\n",
    "            'params': sum(p.numel() for p in model.parameters()),\n",
    "        },\n",
    "        'audio2flame': {\n",
    "            'class': 'Audio2FLAMETransformer',\n",
    "            'd_model': 512,\n",
    "            'nhead': 8,\n",
    "            'num_layers': 6,\n",
    "            'n_flame_params': 53,\n",
    "            'params': sum(p.numel() for p in audio_model.parameters()),\n",
    "        },\n",
    "        'pipeline': {\n",
    "            'input': 'audio_wav (16kHz) + source_image (512x512)',\n",
    "            'output': 'rendered_frames (512x512x3, 25fps)',\n",
    "            'stages': [\n",
    "                '1. DECA: image â†’ FLAME shape params',\n",
    "                '2. Wav2Vec2: audio â†’ features (1024-dim)',\n",
    "                '3. Audio2FLAME: features â†’ expression params (53-dim/frame)',\n",
    "                '4. QuantumHead: Z_id + Z_exp â†’ UV Gaussian maps (14ch, 256Â²)',\n",
    "                '5. GaussianRenderer: UV maps â†’ rendered image (512Â²)',\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    with open(os.path.join(pkg_dir, 'architecture.json'), 'w') as f:\n",
    "        json.dump(arch_info, f, indent=2)\n",
    "\n",
    "    # 4. Package size\n",
    "    total_size = 0\n",
    "    for f_name in os.listdir(pkg_dir):\n",
    "        size = os.path.getsize(os.path.join(pkg_dir, f_name))\n",
    "        total_size += size\n",
    "        print(f'  {f_name}: {size/1e6:.1f} MB')\n",
    "\n",
    "    print(f'\\nâœ“ Inference package: {pkg_dir} ({total_size/1e6:.1f} MB total)')\n",
    "    return pkg_dir\n",
    "\n",
    "\n",
    "# Export\n",
    "pkg_dir = export_inference_package(\n",
    "    model, audio_model, config, OUTPUT_DIR\n",
    ")\n",
    "\n",
    "# Push the package\n",
    "print('\\nPushing inference package to spike2...')\n",
    "for f_name in os.listdir(pkg_dir):\n",
    "    f_path = os.path.join(pkg_dir, f_name)\n",
    "    push_to_spike2(f_path, SPIKE2_HOST, SPIKE2_WEIGHTS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7f0881",
   "metadata": {},
   "source": [
    "## 12. Test Inference (via spike2 API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42c906b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TEST INFERENCE VIA SPIKE2\n",
    "# ============================================================\n",
    "import requests\n",
    "import time\n",
    "import base64\n",
    "from IPython.display import display, Image as IPImage, Video\n",
    "\n",
    "\n",
    "def test_quantumhead_inference(text, source_image_path=None):\n",
    "    \"\"\"Test the full pipeline via spike2 API.\n",
    "\n",
    "    1. TTS: text â†’ audio\n",
    "    2. QuantumHead: audio + image â†’ video\n",
    "    \"\"\"\n",
    "    base_url = f'https://{SPIKE2_HOST}'\n",
    "\n",
    "    # Step 1: Generate speech audio\n",
    "    print('Step 1: Generating speech...')\n",
    "    tts_resp = requests.post(\n",
    "        f'{base_url}/speak',\n",
    "        json={\n",
    "            'text': text,\n",
    "            'voice_id': '960f89fc',  # Riley's voice\n",
    "        },\n",
    "        timeout=60,\n",
    "        verify=False,\n",
    "    )\n",
    "    if not tts_resp.ok:\n",
    "        print(f'TTS failed: {tts_resp.status_code}')\n",
    "        return\n",
    "\n",
    "    audio_data = tts_resp.json()\n",
    "    audio_url = audio_data.get('audio_url', audio_data.get('url'))\n",
    "    print(f'  Audio: {audio_url}')\n",
    "\n",
    "    # Step 2: Generate avatar video via QuantumHead\n",
    "    print('Step 2: Generating avatar video...')\n",
    "    qh_payload = {\n",
    "        'audio_url': audio_url,\n",
    "        'model': 'quantumhead',\n",
    "    }\n",
    "    if source_image_path:\n",
    "        with open(source_image_path, 'rb') as f:\n",
    "            qh_payload['source_image'] = base64.b64encode(f.read()).decode()\n",
    "\n",
    "    qh_resp = requests.post(\n",
    "        f'{base_url}/quantumhead/generate',\n",
    "        json=qh_payload,\n",
    "        timeout=120,\n",
    "        verify=False,\n",
    "    )\n",
    "\n",
    "    if qh_resp.ok:\n",
    "        result = qh_resp.json()\n",
    "        video_url = result.get('video_url')\n",
    "        print(f'  âœ“ Video: {video_url}')\n",
    "\n",
    "        # Download and display\n",
    "        if video_url:\n",
    "            vid_data = requests.get(video_url, verify=False).content\n",
    "            with open('/tmp/quantumhead_test.mp4', 'wb') as f:\n",
    "                f.write(vid_data)\n",
    "            display(Video('/tmp/quantumhead_test.mp4', embed=True, width=512))\n",
    "    else:\n",
    "        print(f'  âœ— Generation failed: {qh_resp.status_code}')\n",
    "        print(f'    {qh_resp.text}')\n",
    "\n",
    "\n",
    "# Test it!\n",
    "test_quantumhead_inference(\n",
    "    \"Hello! I'm a 3D Gaussian Splatting avatar powered by QuantumHead. \"\n",
    "    \"Built with FLAME parametric models and trained on an A100 GPU.\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

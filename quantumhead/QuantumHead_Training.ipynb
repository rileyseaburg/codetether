{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86a0a75d",
   "metadata": {},
   "source": [
    "# QuantumHead \u2014 3DGS Talking Head Avatar\n",
    "\n",
    "**Train on Colab A100 \u2192 Export weights to GCS \u2192 Serve on spike2 (8GB GPU)**\n",
    "\n",
    "Pipeline:\n",
    "1. Install deps (verified for Colab Python 3.12 + CUDA 13.0)\n",
    "2. Download FLAME model + pretrained face reconstruction\n",
    "3. Upload selfies \u2192 FLAME fitting \u2192 dataset\n",
    "4. Train QuantumHead (UV-Space Gaussians + Audio2FLAME)\n",
    "5. Export weights to GCS bucket\n",
    "6. Test inference locally\n",
    "\n",
    "Total VRAM at inference: ~2GB fp16 (fits on RTX 2080 SUPER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd407c7",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eacc85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "import os\n",
    "\n",
    "# GCS bucket for weight transfer (Colab \u2192 GCS \u2192 spike2)\n",
    "GCS_BUCKET = \"veo-spotless\"\n",
    "GCS_PREFIX = \"quantumhead/weights\"\n",
    "GCS_PROJECT = \"spotlessbinco\"\n",
    "\n",
    "# Training config\n",
    "BATCH_SIZE = 4\n",
    "NUM_GAUSSIANS = 256_000\n",
    "UV_MAP_SIZE = 256\n",
    "EXPRESSION_DIM = 256\n",
    "IDENTITY_DIM = 512\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_ITERATIONS = 50_000\n",
    "GUIDE_MESH_VERTICES = 5023  # FLAME vertex count\n",
    "\n",
    "# Paths\n",
    "OUTPUT_DIR = \"/content/quantumhead_output\"\n",
    "CHECKPOINT_DIR = f\"{OUTPUT_DIR}/checkpoints\"\n",
    "DATA_ROOT = \"/content/quantumhead_data\"\n",
    "WEIGHTS_DIR = \"/content/weights\"\n",
    "SELFIE_DIR = f\"{DATA_ROOT}/selfies\"\n",
    "FLAME_DIR = f\"{DATA_ROOT}/flame_params\"\n",
    "\n",
    "for d in [OUTPUT_DIR, CHECKPOINT_DIR, DATA_ROOT, WEIGHTS_DIR, SELFIE_DIR, FLAME_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(\"\u2713 Configuration set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472f9b49",
   "metadata": {},
   "source": [
    "## 2. GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf27712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, torch\n",
    "\n",
    "result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"VRAM: {vram_gb:.1f} GB\")\n",
    "    assert vram_gb > 30, f\"Need A100 (40/80GB), got {vram_gb:.1f}GB. Change Runtime \u2192 A100.\"\n",
    "    print(\"\u2713 A100 confirmed\")\n",
    "else:\n",
    "    raise RuntimeError(\"No GPU! Go to Runtime \u2192 Change runtime type \u2192 A100.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58d6332",
   "metadata": {},
   "source": [
    "## 3. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fa94af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INSTALL DEPENDENCIES \u2014 verified for Colab A100 (Feb 2026)\n",
    "# Colab pre-installs PyTorch, so we only add what's missing\n",
    "# ============================================================\n",
    "\n",
    "# Install in groups to isolate failures\n",
    "import subprocess, sys\n",
    "\n",
    "def pip_install(*pkgs, **kwargs):\n",
    "    \"\"\"Install packages, return success/failure.\"\"\"\n",
    "    extra = kwargs.get('extra_args', [])\n",
    "    cmd = [sys.executable, '-m', 'pip', 'install', '-q'] + list(extra) + list(pkgs)\n",
    "    r = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    if r.returncode != 0:\n",
    "        print(f\"  \u26a0 Failed: {' '.join(pkgs)}\")\n",
    "        if 'error' in r.stderr.lower():\n",
    "            # Print just the error line\n",
    "            for line in r.stderr.split('\\n'):\n",
    "                if 'error' in line.lower():\n",
    "                    print(f\"    {line.strip()}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "print(\"Installing dependencies...\")\n",
    "\n",
    "# Group 1: Core 3D/rendering (no pytorch3d \u2014 too fragile, we don't need it)\n",
    "print(\"  [1/6] 3D & rendering...\")\n",
    "pip_install('trimesh', 'plyfile', 'einops')\n",
    "\n",
    "# Group 2: Face reconstruction\n",
    "print(\"  [2/6] Face reconstruction...\")\n",
    "pip_install('face-alignment', 'mediapipe')\n",
    "# insightface needs specific onnxruntime\n",
    "pip_install('insightface', 'onnxruntime-gpu')\n",
    "\n",
    "# Group 3: Audio\n",
    "print(\"  [3/6] Audio processing...\")\n",
    "pip_install('transformers', 'librosa', 'soundfile')\n",
    "\n",
    "# Group 4: Diffusion & training utils\n",
    "print(\"  [4/6] Training utils...\")\n",
    "pip_install('diffusers', 'accelerate', 'lpips', 'wandb')\n",
    "\n",
    "# Group 5: GCS + helpers\n",
    "print(\"  [5/6] GCS & helpers...\")\n",
    "pip_install('google-cloud-storage', 'gdown', 'huggingface_hub', 'gtts')\n",
    "\n",
    "# Group 6: OpenCV, scikit-image\n",
    "print(\"  [6/6] Image processing...\")\n",
    "pip_install('opencv-python-headless', 'scikit-image')\n",
    "\n",
    "# Verify critical imports\n",
    "print(\"\\nVerifying imports...\")\n",
    "critical = ['torch', 'cv2', 'numpy', 'einops', 'transformers', 'librosa', 'tqdm']\n",
    "for mod in critical:\n",
    "    try:\n",
    "        __import__(mod)\n",
    "        print(f\"  \u2713 {mod}\")\n",
    "    except ImportError:\n",
    "        print(f\"  \u2717 {mod} \u2014 MISSING\")\n",
    "\n",
    "print(\"\\n\u2713 Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3f036c",
   "metadata": {},
   "source": [
    "## 4. Download FLAME Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bc0662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DOWNLOAD FLAME MODEL\n",
    "# Source 1: GCS bucket (most reliable \u2014 pre-uploaded)\n",
    "# Source 2: gsutil CLI fallback\n",
    "# Source 3: Manual upload\n",
    "# ============================================================\n",
    "import os\n",
    "\n",
    "FLAME_MODEL_PATH = f\"{WEIGHTS_DIR}/generic_model.pkl\"\n",
    "\n",
    "if os.path.exists(FLAME_MODEL_PATH):\n",
    "    print(f\"\u2713 FLAME already exists: {FLAME_MODEL_PATH}\")\n",
    "else:\n",
    "    print(\"Downloading FLAME model...\")\n",
    "    downloaded = False\n",
    "\n",
    "    # Authenticate with GCS first\n",
    "    try:\n",
    "        from google.colab import auth\n",
    "        auth.authenticate_user()\n",
    "        print(\"  \u2713 GCS authenticated\")\n",
    "    except Exception as e:\n",
    "        print(f\"  GCS auth warning: {e}\")\n",
    "\n",
    "    # Source 1: GCS Python API\n",
    "    try:\n",
    "        from google.cloud import storage\n",
    "        client = storage.Client(project=GCS_PROJECT)\n",
    "        bucket = client.bucket(GCS_BUCKET)\n",
    "        blob = bucket.blob(f\"{GCS_PREFIX}/generic_model.pkl\")\n",
    "        if blob.exists():\n",
    "            blob.download_to_filename(FLAME_MODEL_PATH)\n",
    "            downloaded = os.path.exists(FLAME_MODEL_PATH) and os.path.getsize(FLAME_MODEL_PATH) > 1_000_000\n",
    "            if downloaded:\n",
    "                print(f\"  \u2713 FLAME from GCS: {FLAME_MODEL_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  GCS API failed: {e}\")\n",
    "\n",
    "    # Source 2: gsutil CLI (pre-authenticated on Colab)\n",
    "    if not downloaded:\n",
    "        try:\n",
    "            import subprocess\n",
    "            r = subprocess.run(\n",
    "                ['gsutil', 'cp', f'gs://{GCS_BUCKET}/{GCS_PREFIX}/generic_model.pkl', FLAME_MODEL_PATH],\n",
    "                capture_output=True, text=True, timeout=120\n",
    "            )\n",
    "            downloaded = os.path.exists(FLAME_MODEL_PATH) and os.path.getsize(FLAME_MODEL_PATH) > 1_000_000\n",
    "            if downloaded:\n",
    "                print(f\"  \u2713 FLAME via gsutil\")\n",
    "            else:\n",
    "                print(f\"  gsutil failed: {r.stderr[:200]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  gsutil failed: {e}\")\n",
    "\n",
    "    # Source 3: Manual upload\n",
    "    if not downloaded:\n",
    "        print(\"  \u26a0 Auto-download failed. Upload generic_model.pkl:\")\n",
    "        try:\n",
    "            from google.colab import files as colab_files\n",
    "            print(\"    Click upload button below:\")\n",
    "            uploaded = colab_files.upload()\n",
    "            for name, data in uploaded.items():\n",
    "                if 'generic_model' in name or name.endswith('.pkl'):\n",
    "                    with open(FLAME_MODEL_PATH, 'wb') as f:\n",
    "                        f.write(data)\n",
    "                    downloaded = True\n",
    "                    print(f\"  \u2713 Uploaded: {name}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "        if not downloaded:\n",
    "            print(f\"    Move file to: {FLAME_MODEL_PATH}\")\n",
    "\n",
    "# Verify FLAME\n",
    "if os.path.exists(FLAME_MODEL_PATH):\n",
    "    size_mb = os.path.getsize(FLAME_MODEL_PATH) / 1e6\n",
    "    import pickle\n",
    "    with open(FLAME_MODEL_PATH, 'rb') as f:\n",
    "        flame_data = pickle.load(f, encoding='latin1')\n",
    "    n_verts = flame_data['v_template'].shape[0]\n",
    "    n_faces = flame_data['f'].shape[0]\n",
    "    print(f\"  FLAME model: {size_mb:.1f} MB \u2014 {n_verts} vertices, {n_faces} faces\")\n",
    "    print(\"\u2713 FLAME model verified\")\n",
    "else:\n",
    "    print(\"\u2717 FLAME model not found \u2014 cannot proceed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a668e0d8",
   "metadata": {},
   "source": [
    "## 5. Upload Source Images\n",
    "\n",
    "Upload your selfies. You need at least 3-5 photos of your face from different angles.\n",
    "Methods (in order of preference):\n",
    "1. **GCS bucket** \u2014 if you already uploaded selfies to `gs://veo-spotless/quantumhead/selfies/`\n",
    "2. **Direct upload** \u2014 use the Colab file picker\n",
    "3. **Webcam** \u2014 capture directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21c3e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD SOURCE IMAGES\n",
    "# ============================================================\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "os.makedirs(SELFIE_DIR, exist_ok=True)\n",
    "\n",
    "# --- Method 1: GCS bucket ---\n",
    "try:\n",
    "    from google.colab import auth\n",
    "    from google.cloud import storage\n",
    "    auth.authenticate_user()\n",
    "    client = storage.Client(project=GCS_PROJECT)\n",
    "    bucket = client.bucket(GCS_BUCKET)\n",
    "    blobs = list(bucket.list_blobs(prefix='quantumhead/selfies'))\n",
    "    img_blobs = [b for b in blobs if b.name.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "    if img_blobs:\n",
    "        for blob in img_blobs:\n",
    "            fname = os.path.basename(blob.name)\n",
    "            if fname:  # skip directory entries\n",
    "                dest = os.path.join(SELFIE_DIR, fname)\n",
    "                blob.download_to_filename(dest)\n",
    "        print(f\"\u2713 Downloaded {len(img_blobs)} selfies from GCS\")\n",
    "    else:\n",
    "        print(\"No selfies in GCS bucket, trying direct upload...\")\n",
    "except Exception as e:\n",
    "    print(f\"GCS not available ({e}), trying direct upload...\")\n",
    "\n",
    "# --- Method 2: Direct upload ---\n",
    "n_existing = len(list(Path(SELFIE_DIR).glob('*.jpg'))) + len(list(Path(SELFIE_DIR).glob('*.png')))\n",
    "if n_existing == 0:\n",
    "    try:\n",
    "        from google.colab import files as colab_files\n",
    "        print(\"\\nUpload your selfies (JPG/PNG, at least 3-5 photos):\")\n",
    "        uploaded = colab_files.upload()\n",
    "        for name, data in uploaded.items():\n",
    "            dest = os.path.join(SELFIE_DIR, name)\n",
    "            with open(dest, 'wb') as f:\n",
    "                f.write(data)\n",
    "            print(f\"  \u2713 {name}\")\n",
    "    except Exception:\n",
    "        print(\"Direct upload not available in this environment\")\n",
    "\n",
    "# Count\n",
    "n_images = len(list(Path(SELFIE_DIR).glob('*.jpg'))) + \\\n",
    "           len(list(Path(SELFIE_DIR).glob('*.png'))) + \\\n",
    "           len(list(Path(SELFIE_DIR).glob('*.jpeg')))\n",
    "print(f\"\\n{'\u2713' if n_images >= 3 else '\u26a0'} {n_images} source images in {SELFIE_DIR}\")\n",
    "if n_images == 0:\n",
    "    print(\"  Upload selfies before proceeding!\")\n",
    "elif n_images < 3:\n",
    "    print(\"  Recommend at least 3-5 photos for quality results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3221fc85",
   "metadata": {},
   "source": [
    "## 6. FLAME Parametric Head Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f06fc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FLAME MODEL \u2014 Parametric Head\n",
    "# shape(\u03b2): 300-dim identity, expression(\u03c8): 100-dim, pose(\u03b8): 15-dim\n",
    "# ============================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "class FLAMEModel(nn.Module):\n",
    "    def __init__(self, flame_path, n_shape=300, n_exp=100):\n",
    "        super().__init__()\n",
    "        with open(flame_path, 'rb') as f:\n",
    "            flame_data = pickle.load(f, encoding='latin1')\n",
    "\n",
    "        self.register_buffer('v_template', torch.tensor(\n",
    "            np.array(flame_data['v_template']), dtype=torch.float32))\n",
    "        self.register_buffer('shapedirs', torch.tensor(\n",
    "            np.array(flame_data['shapedirs'][:, :, :n_shape]), dtype=torch.float32))\n",
    "        self.register_buffer('exprdirs', torch.tensor(\n",
    "            np.array(flame_data['shapedirs'][:, :, 300:300+n_exp]), dtype=torch.float32))\n",
    "\n",
    "        # Joint regressor\n",
    "        J = flame_data['J_regressor']\n        J_regressor = np.array(J.todense()) if hasattr(J, 'todense') else np.array(J)\n",
    "        self.register_buffer('J_regressor', torch.tensor(J_regressor, dtype=torch.float32))\n",
    "\n",
    "        # Skinning weights for LBS\n",
    "        self.register_buffer('lbs_weights', torch.tensor(\n",
    "            np.array(flame_data['weights']), dtype=torch.float32))\n",
    "\n",
    "        # Faces\n",
    "        self.register_buffer('faces_tensor', torch.tensor(\n",
    "            np.array(flame_data['f']).astype(np.int64)))\n",
    "\n",
    "        self.n_vertices = self.v_template.shape[0]\n",
    "        self.n_shape = n_shape\n",
    "        self.n_exp = n_exp\n",
    "\n",
    "    def forward(self, shape_params, expression_params):\n",
    "        \"\"\"Simplified forward: shape+expression blendshapes (no pose/LBS for training speed).\"\"\"\n",
    "        v = self.v_template.unsqueeze(0) + \\\n",
    "            torch.einsum('bl,mkl->bmk', shape_params, self.shapedirs) + \\\n",
    "            torch.einsum('bl,mkl->bmk', expression_params, self.exprdirs)\n",
    "        return v  # (B, 5023, 3)\n",
    "\n",
    "\n",
    "# Test\n",
    "flame = FLAMEModel(FLAME_MODEL_PATH).cuda()\n",
    "dummy_shape = torch.zeros(1, 300).cuda()\n",
    "dummy_exp = torch.zeros(1, 100).cuda()\n",
    "verts = flame(dummy_shape, dummy_exp)\n",
    "print(f\"\u2713 FLAME model loaded: {verts.shape} vertices\")\n",
    "del dummy_shape, dummy_exp, verts\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bee5bd",
   "metadata": {},
   "source": [
    "## 7. FLAME Fitting (Image \u2192 Parameters)\n",
    "\n",
    "Fits FLAME shape/expression/pose to each selfie using `face-alignment` landmarks.\n",
    "No DECA dependency \u2014 uses a direct landmark-to-FLAME optimization loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdf5a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FLAME FITTING \u2014 Optimize FLAME params to match face landmarks\n",
    "# Uses face_alignment library for 68-point landmarks\n",
    "# ============================================================\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import cv2\n",
    "import face_alignment\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "fa = face_alignment.FaceAlignment(face_alignment.LandmarksType.TWO_D, device='cuda')\n",
    "\n",
    "# FLAME landmark indices (subset of 5023 vertices \u2192 68 face landmarks)\n",
    "# Standard 68 landmark mapping to FLAME vertex indices\n",
    "FLAME_LMK_IDX = [\n",
    "    # Jaw contour (0-16)\n",
    "    3564, 3500, 3418, 3332, 3246, 3165, 3084, 2994, 2907,\n",
    "    2820, 2737, 2654, 2575, 2496, 2421, 2352, 2289,\n",
    "    # Right eyebrow (17-21)\n",
    "    3863, 3853, 3839, 3822, 3800,\n",
    "    # Left eyebrow (22-26)\n",
    "    2178, 2196, 2218, 2237, 2259,\n",
    "    # Nose bridge (27-30)\n",
    "    3541, 3529, 3510, 3496,\n",
    "    # Nose bottom (31-35)\n",
    "    2370, 2390, 2412, 2432, 2454,\n",
    "    # Right eye (36-41)\n",
    "    3716, 3722, 3737, 3750, 3744, 3729,\n",
    "    # Left eye (42-47)\n",
    "    2088, 2094, 2109, 2122, 2116, 2101,\n",
    "    # Outer mouth (48-59)\n",
    "    1694, 1700, 1722, 1736, 1750, 1772, 1778, 1770, 1756, 1740, 1724, 1706,\n",
    "    # Inner mouth (60-67)\n",
    "    1708, 1726, 1738, 1752, 1768, 1754, 1742, 1728,\n",
    "]\n",
    "\n",
    "\n",
    "def fit_flame_to_image(image_path, flame_model, n_iters=200, lr=0.01):\n",
    "    \"\"\"Fit FLAME params to a single image via landmark optimization.\"\"\"\n",
    "    img = cv2.imread(str(image_path))\n",
    "    if img is None:\n",
    "        return None\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Detect landmarks\n",
    "    lmks = fa.get_landmarks(img_rgb)\n",
    "    if lmks is None or len(lmks) == 0:\n",
    "        print(f\"  No face detected in {image_path.name}\")\n",
    "        return None\n",
    "    lmks_2d = torch.tensor(lmks[0], dtype=torch.float32).cuda()  # (68, 2)\n",
    "\n",
    "    # Normalize landmarks to [-1, 1]\n",
    "    h, w = img.shape[:2]\n",
    "    lmks_2d[:, 0] = (lmks_2d[:, 0] / w) * 2 - 1\n",
    "    lmks_2d[:, 1] = (lmks_2d[:, 1] / h) * 2 - 1\n",
    "\n",
    "    # Optimize FLAME params\n",
    "    shape = torch.zeros(1, 300, device='cuda', requires_grad=True)\n",
    "    exp = torch.zeros(1, 100, device='cuda', requires_grad=True)\n",
    "\n",
    "    optimizer = torch.optim.Adam([shape, exp], lr=lr)\n",
    "\n",
    "    for i in range(n_iters):\n",
    "        optimizer.zero_grad()\n",
    "        verts = flame_model(shape, exp)  # (1, 5023, 3)\n",
    "\n",
    "        # Project: take landmark vertices, use xy as 2D projection (orthographic)\n",
    "        pred_lmks = verts[0, FLAME_LMK_IDX, :2]  # (68, 2)\n",
    "\n",
    "        loss = F.mse_loss(pred_lmks, lmks_2d)\n",
    "\n",
    "        # Regularization\n",
    "        loss = loss + 0.001 * (shape ** 2).sum() + 0.0001 * (exp ** 2).sum()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return {\n",
    "        'shape': shape.detach().cpu().numpy().flatten(),\n",
    "        'expression': exp.detach().cpu().numpy().flatten(),\n",
    "        'loss': loss.item(),\n",
    "    }\n",
    "\n",
    "\n",
    "# Fit all selfies\n",
    "print(f\"Fitting FLAME to selfies in {SELFIE_DIR}...\")\n",
    "selfie_paths = sorted(list(Path(SELFIE_DIR).glob('*.jpg')) +\n",
    "                       list(Path(SELFIE_DIR).glob('*.png')) +\n",
    "                       list(Path(SELFIE_DIR).glob('*.jpeg')))\n",
    "\n",
    "if len(selfie_paths) == 0:\n",
    "    print(\"\u26a0 No selfies found! Go back to step 5 and upload images.\")\n",
    "else:\n",
    "    for img_path in tqdm(selfie_paths):\n",
    "        result = fit_flame_to_image(img_path, flame)\n",
    "        if result is not None:\n",
    "            out_path = Path(FLAME_DIR) / f\"{img_path.stem}.npz\"\n",
    "            np.savez(str(out_path), **result)\n",
    "            print(f\"  \u2713 {img_path.name} \u2192 loss={result['loss']:.6f}\")\n",
    "\n",
    "    n_fitted = len(list(Path(FLAME_DIR).glob('*.npz')))\n",
    "    print(f\"\\n\u2713 FLAME fitting complete: {n_fitted}/{len(selfie_paths)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e86a7d",
   "metadata": {},
   "source": [
    "## 8. QuantumHead Model (UV-Space Gaussians)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf46b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# UV-SPACE GAUSSIAN AVATAR MODEL\n",
    "# Architecture: GAGAvatar + UHAP patterns\n",
    "# ============================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.norm = nn.InstanceNorm2d(out_ch)\n",
    "        self.act = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.norm(self.conv(x)))\n",
    "\n",
    "\n",
    "class UVDecoder(nn.Module):\n",
    "    \"\"\"Decodes latent \u2192 UV-space Gaussian attribute maps.\n",
    "\n",
    "    Output channels: position_offset(3) + rotation(4) + scale(3) + opacity(1) + color(3) = 14\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim, uv_size=256, out_channels=14):\n",
    "        super().__init__()\n",
    "        self.uv_size = uv_size\n",
    "        self.init_size = 4  # start from 4x4\n",
    "        self.fc = nn.Linear(z_dim, 256 * self.init_size * self.init_size)\n",
    "\n",
    "        # Upsample: 4\u21928\u219216\u219232\u219264\u2192128\u2192256 (6 blocks)\n",
    "        channels = [256, 256, 128, 128, 64, 64]\n",
    "        self.blocks = nn.ModuleList()\n",
    "        in_ch = 256\n",
    "        for out_ch in channels:\n",
    "            self.blocks.append(nn.Sequential(\n",
    "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "                ConvBlock(in_ch, out_ch),\n",
    "                ConvBlock(out_ch, out_ch),\n",
    "            ))\n",
    "            in_ch = out_ch\n",
    "        self.head = nn.Conv2d(channels[-1], out_channels, 1)\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.fc(z).view(-1, 256, self.init_size, self.init_size)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return self.head(x)  # (B, 14, uv_size, uv_size)\n",
    "\n",
    "\n",
    "class ExpressionEncoder(nn.Module):\n",
    "    \"\"\"Encodes UV attribute difference maps \u2192 expression latent Z_exp.\"\"\"\n",
    "    def __init__(self, in_channels=14, z_dim=256, uv_size=256):\n",
    "        super().__init__()\n",
    "        # Downsample: 256\u2192128\u219264\u219232\u219216\u21928\u21924\n",
    "        channels = [32, 64, 64, 128, 128, 256]\n",
    "        layers = []\n",
    "        in_ch = in_channels\n",
    "        for out_ch in channels:\n",
    "            layers.extend([\n",
    "                nn.Conv2d(in_ch, out_ch, 4, 2, 1),\n",
    "                nn.InstanceNorm2d(out_ch),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "            ])\n",
    "            in_ch = out_ch\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        self.fc_mu = nn.Linear(256 * 4 * 4, z_dim)\n",
    "        self.fc_logvar = nn.Linear(256 * 4 * 4, z_dim)\n",
    "\n",
    "    def forward(self, uv_diff):\n",
    "        h = self.encoder(uv_diff).flatten(1)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        # Reparameterize\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            z = mu + std * torch.randn_like(std)\n",
    "        else:\n",
    "            z = mu\n",
    "        return z, mu, logvar\n",
    "\n",
    "\n",
    "class QuantumHeadModel(nn.Module):\n",
    "    \"\"\"Full model: Z_id + Z_exp \u2192 UV Gaussian maps + guide mesh offsets.\"\"\"\n",
    "    def __init__(self, z_id_dim=512, z_exp_dim=256, uv_size=256, n_vertices=5023):\n",
    "        super().__init__()\n",
    "        self.z_id_dim = z_id_dim\n",
    "        self.z_exp_dim = z_exp_dim\n",
    "\n",
    "        # Identity decoder (neutral canonical appearance)\n",
    "        self.neutral_decoder = UVDecoder(z_id_dim, uv_size, out_channels=14)\n",
    "\n",
    "        # Expression-conditioned decoder (deformation from neutral)\n",
    "        self.expr_decoder = UVDecoder(z_id_dim + z_exp_dim, uv_size, out_channels=14)\n",
    "\n",
    "        # Expression encoder (for VAE training)\n",
    "        self.expr_encoder = ExpressionEncoder(14, z_exp_dim, uv_size)\n",
    "\n",
    "        # Guide mesh decoder\n",
    "        self.mesh_decoder = nn.Sequential(\n",
    "            nn.Linear(z_id_dim + z_exp_dim, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(2048, n_vertices * 3),\n",
    "        )\n",
    "        self.n_vertices = n_vertices\n",
    "\n",
    "    def forward(self, z_id, z_exp):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            uv_maps: (B, 14, K, K) \u2014 Gaussian attribute maps\n",
    "            mesh_offsets: (B, V, 3) \u2014 vertex offsets from template\n",
    "            z_exp, mu, logvar: expression latent (for KL loss)\n",
    "        \"\"\"\n",
    "        # Neutral (identity-only)\n",
    "        uv_neutral = self.neutral_decoder(z_id)\n",
    "\n",
    "        # Expression delta\n",
    "        z_combined = torch.cat([z_id, z_exp], dim=-1)\n",
    "        uv_delta = self.expr_decoder(z_combined)\n",
    "\n",
    "        # Final UV maps = neutral + delta\n",
    "        uv_maps = uv_neutral + uv_delta\n",
    "\n",
    "        # Guide mesh offsets\n",
    "        mesh_offsets = self.mesh_decoder(z_combined).view(-1, self.n_vertices, 3)\n",
    "\n",
    "        return uv_maps, mesh_offsets\n",
    "\n",
    "    def encode_expression(self, uv_target, uv_neutral):\n",
    "        \"\"\"Encode expression from UV difference.\"\"\"\n",
    "        diff = uv_target - uv_neutral\n",
    "        z_exp, mu, logvar = self.expr_encoder(diff)\n",
    "        return z_exp, mu, logvar\n",
    "\n",
    "\n",
    "# Test instantiation\n",
    "model = QuantumHeadModel(\n",
    "    z_id_dim=IDENTITY_DIM,\n",
    "    z_exp_dim=EXPRESSION_DIM,\n",
    "    uv_size=UV_MAP_SIZE,\n",
    "    n_vertices=GUIDE_MESH_VERTICES,\n",
    ").cuda()\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters()) / 1e6\n",
    "print(f\"\u2713 QuantumHeadModel: {n_params:.1f}M params\")\n",
    "\n",
    "# Quick forward test\n",
    "z_id = torch.randn(1, IDENTITY_DIM).cuda()\n",
    "z_exp = torch.randn(1, EXPRESSION_DIM).cuda()\n",
    "uv, mesh = model(z_id, z_exp)\n",
    "print(f\"  UV maps: {uv.shape}\")\n",
    "print(f\"  Mesh offsets: {mesh.shape}\")\n",
    "vram_mb = torch.cuda.memory_allocated() / 1e6\n",
    "print(f\"  VRAM used: {vram_mb:.0f} MB\")\n",
    "del z_id, z_exp, uv, mesh\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a18233",
   "metadata": {},
   "source": [
    "## 9. Audio \u2192 FLAME Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912f9595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# AUDIO-TO-FLAME TRANSFORMER\n",
    "# Wav2Vec2 features \u2192 FLAME expression params\n",
    "# ============================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class Audio2FLAMETransformer(nn.Module):\n",
    "    \"\"\"Predicts FLAME expression+jaw params from Wav2Vec2 audio features.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model=512, nhead=8, num_layers=6, n_flame_params=53):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_flame_params = n_flame_params\n",
    "\n",
    "        # Wav2Vec2 output projection (1024 \u2192 d_model)\n",
    "        self.audio_proj = nn.Linear(1024, d_model)\n",
    "\n",
    "        # Positional encoding\n",
    "        pe = torch.zeros(5000, d_model)\n",
    "        position = torch.arange(0, 5000, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "        # Transformer\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=2048,\n",
    "            dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Output head \u2192 53 params (50 expression + 3 jaw pose)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, n_flame_params),\n",
    "        )\n",
    "\n",
    "    def forward(self, audio_features):\n",
    "        \"\"\"\n",
    "        Args: audio_features (B, T, 1024) from Wav2Vec2\n",
    "        Returns: flame_params (B, T, 53)\n",
    "        \"\"\"\n",
    "        B, T, _ = audio_features.shape\n",
    "        x = self.audio_proj(audio_features) + self.pe[:, :T]\n",
    "\n",
    "        # Self-attend with causal mask\n",
    "        mask = nn.Transformer.generate_square_subsequent_mask(T, device=x.device)\n",
    "        out = self.decoder(x, x, tgt_mask=mask)\n",
    "\n",
    "        return self.head(out)  # (B, T, 53)\n",
    "\n",
    "\n",
    "# Test\n",
    "audio_model = Audio2FLAMETransformer().cuda()\n",
    "n_params = sum(p.numel() for p in audio_model.parameters()) / 1e6\n",
    "print(f\"\u2713 Audio2FLAMETransformer: {n_params:.1f}M params\")\n",
    "\n",
    "dummy_audio = torch.randn(1, 50, 1024).cuda()\n",
    "flame_out = audio_model(dummy_audio)\n",
    "print(f\"  Input: {dummy_audio.shape} \u2192 Output: {flame_out.shape}\")\n",
    "del dummy_audio, flame_out\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb2f119",
   "metadata": {},
   "source": [
    "## 10. Gaussian Splatting Renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6442548a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GAUSSIAN RENDERER \u2014 UV maps \u2192 image\n",
    "# Uses differentiable neural rendering (no gsplat dependency)\n",
    "# ============================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class GaussianRenderer(nn.Module):\n",
    "    \"\"\"Differentiable renderer: UV Gaussian maps \u2192 2D image.\n",
    "\n",
    "    For training we use a lightweight neural renderer (2D convolutions).\n",
    "    At inference time this can be swapped for full 3D splatting.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, uv_size=256, image_size=512):\n",
    "        super().__init__()\n",
    "        self.uv_size = uv_size\n",
    "        self.image_size = image_size\n",
    "\n",
    "        # Neural renderer: takes 14-ch UV maps \u2192 RGB image\n",
    "        self.renderer = nn.Sequential(\n",
    "            # UV map processing\n",
    "            ConvBlock(14, 64),\n",
    "            ConvBlock(64, 128),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),  # 256\u2192512\n",
    "            ConvBlock(128, 64),\n",
    "            ConvBlock(64, 32),\n",
    "            nn.Conv2d(32, 3, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, uv_maps):\n",
    "        \"\"\"\n",
    "        Args: uv_maps (B, 14, K, K) UV Gaussian attribute maps\n",
    "        Returns: rendered (B, 3, H, W) RGB image\n",
    "        \"\"\"\n",
    "        return self.renderer(uv_maps)\n",
    "\n",
    "\n",
    "renderer = GaussianRenderer(uv_size=UV_MAP_SIZE, image_size=512).cuda()\n",
    "n_params = sum(p.numel() for p in renderer.parameters()) / 1e6\n",
    "print(f\"\u2713 GaussianRenderer: {n_params:.1f}M params\")\n",
    "\n",
    "# Test render\n",
    "dummy_uv = torch.randn(1, 14, UV_MAP_SIZE, UV_MAP_SIZE).cuda()\n",
    "img = renderer(dummy_uv)\n",
    "print(f\"  UV maps {dummy_uv.shape} \u2192 Image {img.shape}\")\n",
    "del dummy_uv, img\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bb70cf",
   "metadata": {},
   "source": [
    "## 11. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbbf845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATASET \u2014 Face frames + FLAME params\n",
    "# ============================================================\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class FaceDataset(Dataset):\n",
    "    \"\"\"Loads selfie images + their fitted FLAME parameters.\"\"\"\n",
    "\n",
    "    def __init__(self, image_dir, flame_dir, image_size=512):\n",
    "        self.image_size = image_size\n",
    "        self.samples = []\n",
    "\n",
    "        image_dir = Path(image_dir)\n",
    "        flame_dir = Path(flame_dir)\n",
    "\n",
    "        for img_path in sorted(image_dir.glob('*')):\n",
    "            if img_path.suffix.lower() not in ('.jpg', '.jpeg', '.png'):\n",
    "                continue\n",
    "            flame_path = flame_dir / f\"{img_path.stem}.npz\"\n",
    "            if flame_path.exists():\n",
    "                self.samples.append((str(img_path), str(flame_path)))\n",
    "\n",
    "        print(f\"  Dataset: {len(self.samples)} paired samples\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.samples), 1)  # at least 1 for DataLoader\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if len(self.samples) == 0:\n",
    "            return self._dummy()\n",
    "\n",
    "        img_path, flame_path = self.samples[idx % len(self.samples)]\n",
    "\n",
    "        # Load image\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (self.image_size, self.image_size))\n",
    "        img = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0\n",
    "\n",
    "        # Load FLAME params\n",
    "        params = np.load(flame_path)\n",
    "        shape = torch.from_numpy(params['shape'][:300]).float()\n",
    "        expression = torch.from_numpy(params['expression'][:100]).float()\n",
    "\n",
    "        return {'image': img, 'shape': shape, 'expression': expression}\n",
    "\n",
    "    def _dummy(self):\n",
    "        return {\n",
    "            'image': torch.randn(3, self.image_size, self.image_size),\n",
    "            'shape': torch.zeros(300),\n",
    "            'expression': torch.zeros(100),\n",
    "        }\n",
    "\n",
    "\n",
    "dataset = FaceDataset(SELFIE_DIR, FLAME_DIR, image_size=512)\n",
    "dataloader = DataLoader(\n",
    "    dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=2, pin_memory=True, drop_last=True,\n",
    ")\n",
    "print(f\"\u2713 DataLoader ready: {len(dataset)} samples, batch_size={BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9388dc95",
   "metadata": {},
   "source": [
    "## 12. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f960ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAINING LOOP\n",
    "# ============================================================\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "try:\n",
    "    import lpips\n",
    "    lpips_fn = lpips.LPIPS(net='vgg').cuda()\n",
    "    print(\"\u2713 LPIPS perceptual loss loaded\")\n",
    "except ImportError:\n",
    "    lpips_fn = None\n",
    "    print(\"\u26a0 LPIPS not available, using L1 only\")\n",
    "\n",
    "\n",
    "def train(model, renderer, flame_model, dataloader, num_iterations=50000,\n",
    "          lr=1e-4, save_every=5000):\n",
    "    \"\"\"Train QuantumHead model.\"\"\"\n",
    "\n",
    "    device = torch.device('cuda')\n",
    "    model = model.to(device).train()\n",
    "    renderer = renderer.to(device).train()\n",
    "\n",
    "    optimizer = Adam(\n",
    "        list(model.parameters()) + list(renderer.parameters()),\n",
    "        lr=lr\n",
    "    )\n",
    "\n",
    "    # Per-sample identity codes (optimized during training)\n",
    "    n_subjects = max(len(dataset.samples), 1)\n",
    "    z_id_bank = nn.Embedding(n_subjects, IDENTITY_DIM).to(device)\n",
    "    nn.init.normal_(z_id_bank.weight, 0, 0.01)\n",
    "    optimizer.add_param_group({'params': z_id_bank.parameters(), 'lr': lr})\n",
    "\n",
    "    step = 0\n",
    "    pbar = tqdm(total=num_iterations, desc='Training')\n",
    "\n",
    "    while step < num_iterations:\n",
    "        for batch in dataloader:\n",
    "            if step >= num_iterations:\n",
    "                break\n",
    "\n",
    "            images = batch['image'].to(device)            # (B, 3, 512, 512)\n",
    "            shape_params = batch['shape'].to(device)       # (B, 300)\n",
    "            expr_params = batch['expression'].to(device)   # (B, 100)\n",
    "            B = images.shape[0]\n",
    "\n",
    "            # Get identity codes\n",
    "            subject_idx = torch.zeros(B, dtype=torch.long, device=device)\n",
    "            z_id = z_id_bank(subject_idx)  # (B, 512)\n",
    "\n",
    "            # Encode expression from target image (auto-encoder path)\n",
    "            with torch.no_grad():\n",
    "                uv_neutral = model.neutral_decoder(z_id)\n",
    "            # Use FLAME expression as conditioning\n",
    "            # Map FLAME 100-dim \u2192 our 256-dim expression latent\n",
    "            z_exp = F.pad(expr_params[:, :EXPRESSION_DIM], (0, max(0, EXPRESSION_DIM - 100)))\n",
    "\n",
    "            # Forward pass\n",
    "            uv_maps, mesh_offsets = model(z_id, z_exp)\n",
    "\n",
    "            # Render\n",
    "            rendered = renderer(uv_maps)  # (B, 3, 512, 512)\n",
    "\n",
    "            # === Losses ===\n",
    "            # L1 reconstruction\n",
    "            loss_l1 = F.l1_loss(rendered, images)\n",
    "\n",
    "            # Perceptual loss\n",
    "            if lpips_fn is not None:\n",
    "                loss_perc = lpips_fn(rendered * 2 - 1, images * 2 - 1).mean()\n",
    "            else:\n",
    "                loss_perc = torch.tensor(0.0, device=device)\n",
    "\n",
    "            # Regularization\n",
    "            loss_reg = 0.001 * (z_id ** 2).mean()\n",
    "            loss_mesh = 0.01 * (mesh_offsets ** 2).mean()\n",
    "\n",
    "            # Total\n",
    "            loss = loss_l1 + 0.1 * loss_perc + loss_reg + loss_mesh\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Log\n",
    "            if step % 100 == 0:\n",
    "                pbar.set_postfix({\n",
    "                    'L1': f'{loss_l1.item():.4f}',\n",
    "                    'perc': f'{loss_perc.item():.4f}',\n",
    "                    'total': f'{loss.item():.4f}',\n",
    "                })\n",
    "\n",
    "            # Save checkpoint\n",
    "            if step > 0 and step % save_every == 0:\n",
    "                ckpt = {\n",
    "                    'step': step,\n",
    "                    'model': model.state_dict(),\n",
    "                    'renderer': renderer.state_dict(),\n",
    "                    'z_id_bank': z_id_bank.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'config': {\n",
    "                        'z_id_dim': IDENTITY_DIM,\n",
    "                        'z_exp_dim': EXPRESSION_DIM,\n",
    "                        'uv_size': UV_MAP_SIZE,\n",
    "                        'n_vertices': GUIDE_MESH_VERTICES,\n",
    "                    },\n",
    "                }\n",
    "                path = os.path.join(CHECKPOINT_DIR, f'checkpoint_{step:06d}.pt')\n",
    "                torch.save(ckpt, path)\n",
    "                print(f\"\\n  \u2713 Saved checkpoint: {path}\")\n",
    "\n",
    "            step += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    # Save final\n",
    "    final_path = os.path.join(CHECKPOINT_DIR, 'quantumhead_final.pt')\n",
    "    torch.save({\n",
    "        'step': step,\n",
    "        'model': model.state_dict(),\n",
    "        'renderer': renderer.state_dict(),\n",
    "        'z_id_bank': z_id_bank.state_dict(),\n",
    "        'config': {\n",
    "            'z_id_dim': IDENTITY_DIM,\n",
    "            'z_exp_dim': EXPRESSION_DIM,\n",
    "            'uv_size': UV_MAP_SIZE,\n",
    "            'n_vertices': GUIDE_MESH_VERTICES,\n",
    "        },\n",
    "    }, final_path)\n",
    "    print(f\"\\n\u2713 Training complete! Final: {final_path}\")\n",
    "    return final_path\n",
    "\n",
    "print(\"\u2713 Training function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6030b0dd",
   "metadata": {},
   "source": [
    "## 13. Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cba6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# START TRAINING\n",
    "# ============================================================\n",
    "print(f\"Model params:  {sum(p.numel() for p in model.parameters())/1e6:.1f}M\")\n",
    "print(f\"Renderer:      {sum(p.numel() for p in renderer.parameters())/1e6:.1f}M\")\n",
    "print(f\"Dataset:       {len(dataset)} samples\")\n",
    "print(f\"Batch size:    {BATCH_SIZE}\")\n",
    "print(f\"Iterations:    {NUM_ITERATIONS}\")\n",
    "print(f\"Checkpoints:   {CHECKPOINT_DIR}\")\n",
    "print()\n",
    "\n",
    "final_path = train(\n",
    "    model=model,\n",
    "    renderer=renderer,\n",
    "    flame_model=flame,\n",
    "    dataloader=dataloader,\n",
    "    num_iterations=NUM_ITERATIONS,\n",
    "    lr=LEARNING_RATE,\n",
    "    save_every=5000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649c4912",
   "metadata": {},
   "source": [
    "## 14. Upload Weights to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8181522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# UPLOAD TRAINED WEIGHTS TO GCS\n",
    "# ============================================================\n",
    "import os\n",
    "from google.cloud import storage\n",
    "from google.colab import auth\n",
    "\n",
    "auth.authenticate_user()\n",
    "gcs_client = storage.Client(project=GCS_PROJECT)\n",
    "gcs_bucket = gcs_client.bucket(GCS_BUCKET)\n",
    "\n",
    "\n",
    "def upload_to_gcs(local_path, prefix=GCS_PREFIX):\n",
    "    blob_name = f\"{prefix}/{os.path.basename(local_path)}\"\n",
    "    blob = gcs_bucket.blob(blob_name)\n",
    "    blob.upload_from_filename(local_path)\n",
    "    size_mb = os.path.getsize(local_path) / 1e6\n",
    "    print(f\"  \u2713 gs://{GCS_BUCKET}/{blob_name} ({size_mb:.1f} MB)\")\n",
    "    return f\"gs://{GCS_BUCKET}/{blob_name}\"\n",
    "\n",
    "\n",
    "if os.path.exists(final_path):\n",
    "    print(\"Uploading weights to GCS...\")\n",
    "    upload_to_gcs(final_path)\n",
    "    print()\n",
    "    print(\"\u2705 Weights uploaded!\")\n",
    "    print(f\"   Bucket: gs://{GCS_BUCKET}/{GCS_PREFIX}/\")\n",
    "    print()\n",
    "    print(\"To deploy on spike2:\")\n",
    "    print(f\"  gsutil cp gs://{GCS_BUCKET}/{GCS_PREFIX}/*.pt /root/quantumhead/weights/\")\n",
    "    print(\"  curl -X POST http://localhost:8100/quantumhead/pull-weights\")\n",
    "else:\n",
    "    print(\"\u26a0 No checkpoint found. Run training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0924076",
   "metadata": {},
   "source": [
    "## 15. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c07c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TEST INFERENCE \u2014 Generate a talking head video locally\n",
    "# ============================================================\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from IPython.display import display, HTML\n",
    "import base64\n",
    "\n",
    "model.eval()\n",
    "renderer.eval()\n",
    "\n",
    "\n",
    "def generate_test_video(text=\"Hello, I am QuantumHead!\", n_frames=75):\n",
    "    \"\"\"Generate a short test video.\"\"\"\n",
    "    print(f\"Generating {n_frames} frames...\")\n",
    "\n",
    "    # Simple audio-less test: oscillate expression params\n",
    "    z_id = torch.randn(1, IDENTITY_DIM, device='cuda')\n",
    "    frames = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for t in range(n_frames):\n",
    "            # Oscillate expression\n",
    "            phase = t / 25.0  # 25fps\n",
    "            z_exp = torch.zeros(1, EXPRESSION_DIM, device='cuda')\n",
    "            # Animate first few dims (mouth open/close, smile, etc.)\n",
    "            z_exp[0, 0] = 0.5 * np.sin(2 * np.pi * 1.0 * phase)   # jaw\n",
    "            z_exp[0, 1] = 0.3 * np.sin(2 * np.pi * 0.5 * phase)   # lip\n",
    "            z_exp[0, 2] = 0.2 * np.sin(2 * np.pi * 0.3 * phase)   # brow\n",
    "            z_exp[0, 3] = 0.1 * np.sin(2 * np.pi * 0.7 * phase)   # smile\n",
    "\n",
    "            uv_maps, _ = model(z_id, z_exp)\n",
    "            rendered = renderer(uv_maps)\n",
    "\n",
    "            frame = (rendered[0].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "            frames.append(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    # Write video\n",
    "    out_path = f\"{OUTPUT_DIR}/test_inference.mp4\"\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    writer = cv2.VideoWriter(out_path, fourcc, 25, (512, 512))\n",
    "    for f in frames:\n",
    "        writer.write(f)\n",
    "    writer.release()\n",
    "\n",
    "    print(f\"\u2713 Video saved: {out_path}\")\n",
    "\n",
    "    # Display in notebook\n",
    "    with open(out_path, 'rb') as f:\n",
    "        video_bytes = f.read()\n",
    "    b64 = base64.b64encode(video_bytes).decode()\n",
    "    display(HTML(f'''\n",
    "    <video width=\"512\" controls autoplay loop>\n",
    "        <source src=\"data:video/mp4;base64,{b64}\" type=\"video/mp4\">\n",
    "    </video>'''))\n",
    "\n",
    "    # Upload to GCS\n",
    "    try:\n",
    "        upload_to_gcs(out_path)\n",
    "    except Exception as e:\n",
    "        print(f\"  GCS upload skipped: {e}\")\n",
    "\n",
    "    return out_path\n",
    "\n",
    "\n",
    "test_path = generate_test_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c3c389",
   "metadata": {},
   "source": [
    "## 16. Export fp16 Inference Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a896946d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXPORT FP16 INFERENCE PACKAGE (for spike2 8GB GPU)\n",
    "# ============================================================\n",
    "import torch, json, os\n",
    "\n",
    "def export_fp16_package(model, renderer, output_dir):\n",
    "    \"\"\"Export fp16 weights + config for spike2 deployment.\"\"\"\n",
    "    pkg_dir = os.path.join(output_dir, 'inference_package')\n",
    "    os.makedirs(pkg_dir, exist_ok=True)\n",
    "\n",
    "    # Save model in fp16\n",
    "    model_fp16 = {k: v.half() for k, v in model.state_dict().items()}\n",
    "    torch.save(model_fp16, os.path.join(pkg_dir, 'quantumhead_fp16.pt'))\n",
    "\n",
    "    renderer_fp16 = {k: v.half() for k, v in renderer.state_dict().items()}\n",
    "    torch.save(renderer_fp16, os.path.join(pkg_dir, 'renderer_fp16.pt'))\n",
    "\n",
    "    # Config\n",
    "    config = {\n",
    "        'z_id_dim': IDENTITY_DIM,\n",
    "        'z_exp_dim': EXPRESSION_DIM,\n",
    "        'uv_size': UV_MAP_SIZE,\n",
    "        'n_vertices': GUIDE_MESH_VERTICES,\n",
    "        'image_size': 512,\n",
    "        'dtype': 'float16',\n",
    "        'total_params_M': sum(p.numel() for p in model.parameters()) / 1e6,\n",
    "    }\n",
    "    with open(os.path.join(pkg_dir, 'config.json'), 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "    # File sizes\n",
    "    total = 0\n",
    "    for fname in sorted(os.listdir(pkg_dir)):\n",
    "        size = os.path.getsize(os.path.join(pkg_dir, fname))\n",
    "        total += size\n",
    "        print(f\"  {fname}: {size/1e6:.1f} MB\")\n",
    "    print(f\"  Total: {total/1e6:.1f} MB\")\n",
    "\n",
    "    # Upload to GCS\n",
    "    print(\"\\nUploading to GCS...\")\n",
    "    for fname in os.listdir(pkg_dir):\n",
    "        upload_to_gcs(os.path.join(pkg_dir, fname), prefix=f\"{GCS_PREFIX}/inference_package\")\n",
    "\n",
    "    print(f\"\\n\u2713 Inference package at gs://{GCS_BUCKET}/{GCS_PREFIX}/inference_package/\")\n",
    "    return pkg_dir\n",
    "\n",
    "\n",
    "pkg = export_fp16_package(model, renderer, OUTPUT_DIR)\n",
    "print(\"\\n\u2705 Ready for spike2 deployment!\")\n",
    "print(f\"  gsutil -m cp -r gs://{GCS_BUCKET}/{GCS_PREFIX}/inference_package/* /root/quantumhead/weights/\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}